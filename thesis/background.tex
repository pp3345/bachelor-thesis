\chapter{Background \& Related Work}
\label{sec:background}

In this thesis, we present an analysis of the reclocking behavior of a current-generation Intel processor when executing \gls{AVX} instructions. We then use the information obtained from the results of this analysis to build a software prototype that tries to accurately reimplement a subset of the algorithm employed by Intel. This reimplementation is designed to constitute a foundation for exploring possible optimizations to the algorithm that yield a better performance in heterogeneous workloads. In this chapter, we will provide an extensive motivation for our work, describe the technologies we built upon and talk about previous and related work.

\section{Dark Silicon and AVX}
\label{sec:background:motivation}

In 1975, Gordon Moore, one of the co-founders of Intel Corporation, predicted that the transistor density of integrated circuits made of silicon would double roughly every two years \cite{moore1975progress}. Known today as \emph{\gls{moore}}, his prediction held true for several decades with surprising accuracy. However, like every technology, the advancement of silicon-based microprocessors is confined to physical limitations. About thirty years after his famous prophecy, in 2007, Moore projected his law to only have about 10 to 15 years left \cite{moore2007interview}. And again, he was right: the \SI{14}{\nano\meter} process technology Intel used in 2014 yielded an integration density of about 37.5 million transistors per \si{\milli\meter\squared}, whereas Intel's current \SI{10}{\nano\meter} fabrication node from 2018 allows for around 100.8 million transistors within the same area. This means that, over four years, the density has increased only by a factor of 2.688$\times$, whereas -- according to \gls{moore} -- it should have been 4$\times$ \cite{courtland2017intel}.

The second large scaling law of microelectronics is known as \emph{\gls{dennard}} or \emph{Dennardian Scaling} and is named after Robert H. Dennard who is one of the authors of a historically important paper from 1974 on the design of very small \glspl{MOSFET} \cite{dennard1974design}. In their work, \citeauthor{dennard1974design} showed that when a silicon \gls{MOSFET} is scaled down by a factor $\kappa$, its voltage and current requirements decline proportionally with $\kappa$. In turn, this means that as smaller \glspl{MOSFET} can be manufactured with evolving chemical process technology, their power density stays constant, or in other words: as manufacturing possibilities advance, chips of the same total area can be produced with smaller and more transistors and no increases in power density. Similar to \gls{moore}, \gls{dennard} has come to an end in recent years: in the 1970s, current leakage only had very small and negligible impact on a chip's power consumption and was therefore not considered as a component in Dennard's formulas \cite{bohr200730}. However, today a point is reached in the scaling of \glspl{MOSFET} where switching current and threshold voltage are low enough that leakage has become a major source of energy usage in microprocessors, causing increasing power densities and the breakdown of Dennardian scaling.

These scaling laws used to be essential for the advancement of microprocessors as they guaranteed the ability to build more complex and at the same time energy-saving designs over the years. Nowadays, however, doubling the transistors per area also means about doubled power usage if all transistors are run at their full frequency \cite{taylor2012dark}. If power consumption were to stay constant, that would mean that only half of the transistors (in practice, likely half of the available cores) could be active at the same time, whereas the rest would need to stay turned off -- \emph{dark silicon} was coined as a term for this issue. Given the unpleasant outlook, creativity is required from engineers to come up with new ways of improving both power consumption and performance of processors. One approach that has been proposed is the use of \emph{dim silicon} \cite{huang2011scaling}: instead of giving up area to dark silicon, the higher the amount of transistors that are active at the same time, the lower are their frequencies -- either all have their clock speed reduced by the same offset or at least some run slower.

In current Intel x86 processors, we can find a dim silicon approach in their implementation of the \gls{AVX} instruction set extension: as these instructions cause higher energy consumption than previous vector instruction sets or scalar instructions, they may not be executed at full frequency or otherwise system stability would be at risk due to exceeded electrical and thermal limits \cite{inteloptimizationmanual}. In turn, whenever a core of these processors is fed with demanding \gls{AVX} instructions, it will reduce its clock frequency. From this moment on, everything executed on that specific core runs slower, allowing \gls{AVX} instructions to run, but at the cost of reduced performance for other operations. However, frequency changes need some time: \citeauthor{mazouz2014evaluation} \cite{mazouz2014evaluation} have shown them to take between \SI{25}{\micro\second} and \SI{52}{\micro\second} on an Intel Core i7-3770 processor from the \textit{Ivy Bridge} generation. To avoid wasting too much time with frequency switches, a core will keep running at a reduced frequency for a while after the last \gls{AVX} instruction has been executed \cite{inteloptimizationmanual}.

A practical example where vectorization on the \gls{CPU} can be beneficial for performance is \emph{ChaCha20} \cite{bernstein2008chacha}, a stream cipher algorithm for symmetric encryption presented by Daniel J. Bernstein in 2008 that, in the recent years, quickly gained traction as various implementations have been developed and employed in wide-spread commercial products. Google has added support for this algorithm in their Android operating system as well as the Chrome web browser in 2014 \cite{googlechacha20} and, as of June 2016, a cipher suite based on ChaCha20 has become a proposed standard for use in the \gls{TLS} protocol that is commonly used to encrypt internet traffic \cite{rfc7905}. Given the rising adoption of Bernstein's algorithm, it became desirable to create implementations optimized for speed and energy efficiency. \citeauthor{goll2014vectorization} \cite{goll2014vectorization} have presented an \gls{AVX2} implementation of ChaCha20 using \SI{256}{\bit} vectors that provides about doubled performance on an Intel \textit{Haswell} processor compared to an implementation using \gls{SSE} with \SI{128}{\bit} vectors. However, in practice, engineers from Cloudflare \cite{cloudflareinteldangers} have benchmarked the \emph{nginx} web server with a version of the \emph{OpenSSL} cryptographic library that contains an implementation of ChaCha20 with support for \gls{AVX2} and \gls{AVX-512} on an Intel Xeon Silver 4116 processor and found that nginx's throughput is reduced by \SI{10.6}{\percent} compared to when it runs with a variant of OpenSSL without \gls{AVX}. These findings were confirmed by \citeauthor{gottschlag19sfma} \cite{gottschlag19sfma} who measured a \SI{11.4}{\percent} decrease in throughput on an Intel Xeon Gold 6130 processor when comparing nginx with an \gls{AVX-512}-capable implementation of OpenSSL versus an \gls{SSE}4 one.

While at first glance it may seem strange that increased vectorization with \gls{AVX} reduces performance, this behavior is easily explained by the description of Intel's \gls{AVX} implementation above: nginx in combination with an \gls{AVX}-capable build of the OpenSSL library is an example of what we call a \emph{heterogeneous workload} -- a program where only certain parts benefit from vector execution, whereas the rest solely uses scalar instructions. Encryption only takes up a fraction of a web server's total processing time, so only this particular fraction is accelerated through \gls{AVX}. Now, the performance reduction observed when enabling \gls{AVX} originates from the fact that the other part of the program is slowed down due to the attained frequency reduction after \gls{AVX} instructions were executed. This poses a problem for the use of \gls{AVX} in real-world software: only programs that can use vectorization for large parts or nearly all of their code may see benefits, whereas there is a large hazard for developers of software with heterogeneous workloads to cause harm to their program's performance.

\section{Dynamic Voltage and Frequency Scaling}
\label{sec:background:dvfs}

\citeauthor{mittal2014survey} \cite{mittal2014survey} defines \textit{\acrlong{DVFS}} ({\acrshort{DVFS}\glsunset{DVFS}) as a technique \enquote{for altering the voltage and/or frequency of a computing system based on performance and power requirements.} The required electrical power $P$ for switching a \gls{CMOS} gatter can be characterized as $P=\frac{1}{2} \times C \times V^2 \times f$, where $C$ is the circuit's electrical capacitance, $V$ is the voltage, and $f$ is the switching frequency \cite{Hennessy:2017:CAS:3207796}. Looking at the formula where power increases linear in frequency and quadratic in voltage, it becomes clear that the power consumption of a processor can be regulated by controlling these two parameters. Therefore, the idea behind \gls{DVFS} is to have a microprocessor run at high frequencies when system load demands for high performance and at lower frequencies when the system is idling to save energy -- the voltage is set according to what is required to maintain stable execution (i.e., all transistors properly switch within a single clock cycle) at the respective frequencies.

Modern Intel processors have a \gls{DVFS} implementation called \emph{Enhanced Intel SpeedStep Technology} (EIST) \cite{intelsdmsysprogguide}. Using EIST, the operating system may select a \glspl{P-state} for each core -- these \emph{performance states} govern a core's frequency and its voltage. On current-generation processors, \glspl{P-state} simply represent integer multipliers of the chip's bus clock.

Further, the \textit{Skylake} microarchitecture generation introduced in 2015 was the first to support \emph{Intel Speed Shift Technology} \cite{intelsykalekannouncement}, an implementation of the \emph{Collaborative Processor Performance Control} (CPPC) interface defined by the \gls{ACPI} standard \cite{acpispec}. Outside of public marketing, Speed Shift is also called \emph{Hardware-Controlled P-States} (HWP) in Intel's technical documents \cite{intelsdmsysprogguide}. As the name implies, when using HWP, control over the processor's \glspl{P-state} is transferred from the operating system to the hardware itself. Precisely, with \gls{HWP}, a \textit{\acrlong{PCU}} (\acrshort{PCU}\glsunset{PCU}) in the processor constantly monitors load on different parts of the chip as well as power consumption and accordingly assigns voltages and frequencies to the different units within the chip. Contrary to previous possibilities with EIST, the operating system is merely left with the ability to hint the processor about the desired minimum and maximum performance as well as the user's preferences for energy efficiency.

Notably, starting with the \textit{Haswell} microarchitecture (a predecessor of \textit{Skylake}), Intel implemented physical measurement of the power consumption in their processors, whereas previous generations only relied on statistical models. \citeauthor{schuchart2016shift} \cite{schuchart2016shift} have shown these measurements to be very precise and in turn, the \gls{PCU} of these chips is capable of limiting a processor's power consumption nearly exactly at its specified \gls{TDP}. However, as the authors have shown, this introduces deviations in the performance of different chips of the same model up to \SI{5}{\percent} compared to the average due to production fluctuations where some chips require more or less power than others. It is plausible that this variance may be carried over to the results of our analysis when executed on different chips.

In this thesis, we will conduct the \gls{AVX} reclocking analysis with \gls{HWP} enabled as this would be the usual case for real-world systems equipped with current Intel processors. For our software reimplementation, however, we are going to disable \gls{HWP} and make use of the legacy EIST software interface as we need to be able to control the processor's clock speed via means of the operating system.

\section{Power Management}
\label{sec:background:powermanagement}

The issue that motivates our work is, in its essence, related to problems found in the power management of hardware devices \cite{lu2001comparing}. Here, the overall goal is to fulfill a computing system's tasks with minimal power consumption. For example, a \gls{HDD} in a system is only required to be active when there are pending read or write requests, otherwise, hard disks only waste electricity without serving a useful purpose. Thus, vast amounts of energy may be saved by turning them off when they are not needed for a period of time. However, simply spinning disks down as soon as no requests are pending and up again when they arrive is not a feasible strategy as hard disks generally need some time for both operations. In addition, that would potentially waste more power than not shutting them down at all: during spin-up, the motor needs to accelerate the disks from zero to thousands of rotations per minute, which consumes considerable more energy than keeping them running for a period of time when the motor is already at speed.

To estimate whether energy can be saved by turning a hard disk off for a time period, we need to calculate the break-even time $t_{be}$, i.e., the amount of time after which the energy savings from being shut down exceed the energy drawn during the spin-down and spin-up phases. We can simply formulate the equation to find $t_{be}$ as follows:

\begin{displaymath}
\underbrace{p_a \times t_{be}}_{\text{energy consumed when disk stays active}} = \underbrace{e_t + p_s \times (t_{be} - t_t)}_{\text{energy consumed when disk is turned off}}
\end{displaymath}

Here, $p_a$ is the electrical power consumed while a disk is active, $p_s$ is the power consumption while in standby (note that this is not $0$ as some parts of the electronics need to stay active even while the motor is off), $e_t$ is the energy needed during spin-down and spin-up and $t_t$ is the time needed for these transitions. Thus, $t_{be} - t_t$ is the actual time spent in the standby state. Transforming the formula yields

\begin{displaymath}
t_{be} = \frac{e_t - p_s \times t_t}{p_w - p_s}
\end{displaymath}

\noindent as a way of directly calculating $t_{be}$.

An optimal solution to the original problem, however, would require an oracle that foresees the future -- only then, we can know for what period of time a hardware device is going to remain unused, and whether this time is longer than $t_{be}$ for this specific device. So far, no scientifically proven way of precisely predicting the future is known to mankind. Therefore, several approaches using profiling and stochastic models have been proposed in the past and are implemented in modern operating systems. As we do not want to dive deeper into this topic here, see \citeauthor{lu2001comparing} \cite{lu2001comparing} for a more thorough overview of strategies that have been previously studied.

How is this related to the problem statement in this thesis? We do not want to save power, but rather improve performance. Nevertheless, the same thoughts as above also apply to our case. For hard disks, energy can be saved by turning them off in phases where they are not needed -- in our case, performance can potentially be improved by raising the frequency during phases where \gls{AVX} units are not needed. Raising and lowering clock frequencies requires some time, similar to how spin-downs and spin-ups of hard disks take some time. Thus, for our needs, a break-even time like described above exists, too. Further, an oracle would be required for optimal decisions in our case similar to the way it would be required for optimal power management. This also hints that strategies used in power management techniques based on profiling or modeling could perhaps also be applicable to the problem we are investigating.

However, unlike power management researchers, we can not simply test different approaches and algorithms. Power management is generally considered to be a task of an operating system, whereas -- as described in the previous section -- current Intel processors manage frequency scaling (especially, \gls{AVX} reclocking) solely in hardware. This motivates our reimplementation: without it, we could only theorize about what could be possible or not when employing alternative strategies for reclocking. Further, using this reimplementation allows us to implement what is basically an oracle, albeit not applicable for practical applications: when the operating system is handed control over frequency reductions needed for \gls{AVX}, we may also have user-space applications tell us when they are going to stop using \gls{AVX} for a while -- thus, we achieve foresighted knowledge about scalar phases.

%\section{Methodology}
%\label{sec:background:methodology}
%
%The ultimate goal of the work in this thesis is to make \gls{AVX} beneficial for a broader range of applications. Whereas heterogeneous workloads currently often suffer from reduced performance due to lower frequencies during scalar parts and are therefore usually unable to use \gls{AVX}, we hope to find ways of improving this situation with our research. Intel keeps a core running at a lowered frequency after the last \gls{AVX} instruction has retired as they expect that applications are likely to use \gls{AVX} again after a while and because, as described in the previous section, a frequency switch needs some time. However, it is plausible that optimizations to this behavior may exist. For example, what if the processor knew in advance how long scalar and \gls{AVX} phases would last? A break-even point for the length of a scalar phase between two \gls{AVX} phases must exist where the drawbacks of more frequency switches are smaller than the gains from running at a higher frequency, however, as long as the hardware has no indication of how long each phase will last, this potential can not be exploited. A possible approach to gain such knowledge lies in the operating system: most programs expose patterns in their resource usage and as such, an operating system may gather statistical data about the relevant execution phases and use that to forecast durations of future phases. This information may then be communicated to the processor which could use it to estimate whether immediate upclocking after an \gls{AVX} phase could be beneficial to overall performance.
%
%However, it is difficult to evaluate the usefulness of this and other conceivable without modifying the hardware. Even if we had access to an Intel processor's definitions, complex simulations would be required to test alternative algorithms. In this work, we want to construct an alternative that allows to implement and test experimental reclocking governors without touching the hardware. For this purpose, we will start by performing an analysis of the precise behavior of a current Intel processor. In this analysis, we want to find out when exactly a core will change its frequency in response to changed \gls{AVX} load conditions in different situations (e.g., with different amounts of active cores or instruction types). We will gather the wanted information by making use of the processor's \gls{PMU} and by simulating different workloads. Afterwards, we will use the obtained data to create a \emph{reimplementation} of the algorithm within the Linux operating system kernel. Using the framework we implemented for our analysis and a program that allows us to simulate heterogeneous workloads as defined above, we will evaluate how well our reimplementation reflects the algorithm implemented in Intel's hardware and discuss whether this approach is feasible for testing possible optimizations to \gls{APIC} reclocking. Further, we will implement a simple interface that allows user-space programs to tell the operating system's kernel whenever they enter scalar or \gls{AVX} execution phases to govern frequency switches without sacrificing stability and evaluate how that can improve the performance of a heterogeneous workload. Our findings will show that the possibilities for software-based reimplementation suffer from several limitations, making it impossible to provide a conclusive answer to the question of the feasibility of our approach without further work. However, we will also prove that improvements for heterogeneous programs are certainly possible with modified reclocking behavior.

\section{Core Specialization}
\label{sec:background:corespecialization}

The idea behind the approach we follow in this thesis is to optimize the reclocking behavior itself for heterogeneous workloads. \citeauthor{gottschlag19sfma} \cite{gottschlag19sfma} have proposed an alternative method of mitigating the negative effects of frequency reduction during and after \gls{AVX} execution by using \emph{core specialization}. In their work, they categorize both threads and cores into \enquote{AVX} and \enquote{non-AVX} ones. Only AVX cores may then execute AVX threads, whereas AVX code is kept away from the other cores. In turn, only these specific cores experience \gls{AVX}-induced frequency reduction while other cores can keep running at their maximum speed. The authors have built a proof-of-concept implementation using the \gls{MuQSS} where they added specific runqueues for each task type to the scheduler and a system call that allow a thread to communicate to the kernel when it enters and exits \gls{AVX} phases. The modified scheduler implementation then migrates tasks between cores depending on their type.

To evaluate the benefits of this approach, they compared the average frequency of all cores as well as the throughput of an nginx web server with a vectorized implementation of the ChaCha20 algorithm (as previously described in \Cref{sec:background:motivation}) on an Intel Xeon Gold 6130 once when run with an unmodified system and once with enabled core specialization. While unmodified scheduling saw a reduction of \SI{11.2}{\percent} in throughput and \SI{11.4}{\percent} in average frequency compared to nginx without vectorized ChaCha20, their core specialization approach largely alleviated the reductions to a mere \SI{3.2}{\percent} and \SI{4.0}{\percent}, respectively.

Building upon this work, \citeauthor{brantsch19corespecialization} \cite{brantsch19corespecialization} reached a further improvement of the core specialization approach. Instead of relying on user-space to communicate execution phases to the kernel via system calls, he implemented a mechanism called \emph{fault-and-migrate}: it is possible for an operating system to selectively disable \gls{AVX} instructions per core. Then, when a program tries to execute such an instruction, the processor will generate an exception and trap into the kernel. This way, the kernel is notified whenever \gls{AVX} instructions are to be executed and can then migrate the task to a core designated for \gls{AVX} purposes. However, there is no proper notification mechanism available for when a thread has stopped using \gls{AVX}, so different ways of estimating this were implemented. The most promising one turned out to be an approach where a thread is considered to have returned to a scalar phase whenever it does a system call, based on the idea that \gls{AVX} is solely used during phases of numeric computation where there is no need to call the operating system. In case of the nginx benchmark outlined above, this decreased the performance degradation compared to running without \gls{AVX} to just \SI{1}{\percent}. This is a drastic improvement against the \SI{11.2}{\percent} that were originally measured without core specialization, however, performance is still not on par with the variant without \gls{AVX-512}.

Contrary to \citeauthor{brantsch19corespecialization}' approach that defines system calls as the end of \gls{AVX} phases, in this work, we will make estimates by using periodic timer interrupts and counting the executed \gls{AVX} instructions during each period. Whenever the counter is zero, we know that \gls{AVX} was not used in the previous period. This gives us an approximation that is off by one period length in the worst case.