\chapter{Analysis}
\label{sec:analysis}

In order to be able to evaluate potential means of improving Intel's \gls{AVX} reclocking algorithm, we first need to obtain thorough knowledge of the algorithm as it is implemented in current Intel x86 \glspl{CPU}. We can then use this knowledge for the software-based reimplementation presented in \Cref{sec:design} and to understand the hardware-induced constraints Intel needs to keep within, which is in turn necessary for designing a feasible and implementable improved reclocking algorithm.

Intel regularly publishes optimization manuals~\cite{inteloptimizationmanual} intended for compiler developers and software engineers which contain a vague description of the mechanism used for deciding when to lower or raise the processor's frequency upon execution of \gls{AVX} instructions. Precisely, Intel defines three \textit{turbo license levels}, which designate frequency offsets for different instruction mix scenarios:

\begin{itemize}
	\item Level~0: only non-demanding (i.e., scalar, \gls{SSE}, \gls{AVX1} or light \gls{AVX2}) instructions are being executed; a core may run at its maximum turbo frequency. This is the default state.
	\item Level~1: active during the execution of heavy \gls{AVX2} and/or light \gls{AVX-512} instructions. The maximum frequency is lowered to a \gls{SKU}-specific value.
	\item Level~2: used for the execution of heavy \gls{AVX-512} instructions. The maximum frequency is lowered to a \gls{SKU}-specific value that is further below the frequency used in level~1.
\end{itemize}

Here, \enquote{heavy} instructions are defined to be floating-point, integer multiplication or integer \gls{FMA} operations. Given these license levels, Intel states that it may take up to \SI{500}{\micro\second} until the new frequency is applied and about \SI{2}{\milli\second} until a core reverts to level~0 after executing the last \enquote{heavy} instruction. Before the frequency is lowered, a core operates at \enquote{a lower peak capability}, however, Intel does not further specify what that exactly means. Intel hints that the license decisions are not solely bound to the instruction types as given in the level descriptions, but rather depend on the mix of instructions executed within a certain time window.

In this chapter we will describe the design of a framework that allows us to analyze the actual behavior of an x86 processor during the execution of \gls{AVX} instructions. Afterwards, we will present and evaluate the results generated when executed on a system equipped with a modern Intel \gls{CPU}. Finally, we compare our findings to what Intel maintains in their specification and point out deviations of the timings between the actual behavior and their claims.

\section{Methodology}
\label{sec:analysis:methodology}

For our reimplementation, our goal is to create a model of the reclocking behavior of an \gls{AVX-512}-capable \gls{CPU} that is as complete as possible and reflects the decisions made by the hardware with high accuracy. Therefore, by conducting this analysis, we want to answer the following aspects:

\begin{itemize}
	\item When exactly does a \gls{CPU} core decide to reduce or raise it's frequency during and after \gls{AVX} execution?
	\item How much time do turbo license level switches need?
	\item Do the \glspl{CPU} switch directly from level 0 to level 2 in case of heavy \gls{AVX-512} instructions or is there a step to level 1 in between?
	\item What does Intel mean by \enquote{lower peak capability} while lowering the clock?
	\item How complete is Intel's description of the reclocking algorithm?
\end{itemize}

In order to create a precise model we want to analyze these questions in different scenarios, i.e., for different instruction types, for different global load situations as well as with and without enabled turbo frequencies. To reach our goal, we run our analysis framework with synthetic code snippets that are designed to trigger the behavior to be analyzed.

\section{Design}
\label{sec:analysis:design}

Our analysis framework consists of a module for the Linux kernel as well as a user-space component which interact with each other and make use of the \gls{PMU}, a unit commonly found in modern microprocessors that enables software to measure performance and bottlenecks on the hardware level. In the following sections, we will present the design and features of these components and describe how they contribute to our analysis purposes.

\subsection{Performance Monitoring Unit (PMU)}
\label{sec:analysis:design:pmu}
Modern x86 \glspl{CPU} commonly feature a \gls{PMU} \cite{intelsdmsysprogguide} which exposes a set of \textit{performance counters} that may be configured to count assertions of a large set of \textit{performance events}.

Precisely, we use version~3 of the x86 \textit{Architectural Performance Monitoring} facility, which features three \textit{fixed counters} per logical core that count retired instructions, cycles during which the core is not in a halt state and \glsunset{TSC}\gls{TSC} cycles in unhalted state, respectively. The \glsreset{TSC}\gls{TSC} is a simple counter found in current x86 \glspl{CPU} that increments steadily with a fixed frequency, independent of the core clock, thus making it suitable for measuring wall-clock time. In addition to the fixed counters, eight freely configurable counters are available per physical core (four per logical core when \gls{SMT} is enabled). These counters may be set to count any of the performance events available for a specific microarchitecture, e.g., most architectures define events for cache hits/misses, execution stalls or load on specific execution units.

Each counter is represented via a \gls{MSR} and also configured through one. More specifically, software may configure the event to count (non-fixed counters only) and when to count (i.e., in user mode (ring~$\geq$~1) and/or kernel mode (ring~0)). Additionally, the counter can be configured to trigger an interrupt when it overflows. By setting the counter to its maximum value less an offset, this can be used as a mechanism to generate notifications when a certain amount of events of a specific type has occurred. The interrupt vector used for delivery can be configured in the core's \gls{APIC}'s \gls{LVT}. Optionally, the \gls{PMU} may be instructed to freeze all counters at their current values as soon as an interrupt is triggered.

\subsection{Overview}
\label{sec:analysis:design:overview}

The analysis tool presented here is made up of a kernel and a user-space component where the former provides the latter with means to configure the \gls{PMU} and efficient handling for interrupts generated by performance counter overflows.

As depicted in a simplified way in \Cref{fig:analysis:design:overview}, the user-space component spawns $n\in\mathbb{N}$ \textit{execution threads} and $w\in\{1,n\}$ \textit{wait threads}, each corresponding to exactly one execution thread, though not every execution thread must have an associated wait thread. The idea behind having multiple execution threads is to be able to make measurements on multiple cores simultaneously, thereby simulating parallel workloads. Upon execution, each execution thread generates a \gls{PMU} configuration designed to produce the desired measurements, which is then applied by the kernel module. Now, the kernel module jumps back into user-space to an address previously defined by the execution thread which now starts to execute \gls{AVX} instructions until preempted by an overflow interrupt generated by the \gls{PMU} according to its configuration (as described in \Cref{sec:analysis:design:pmu}). Each wait thread is initially suspended until an interrupt is triggered on its corresponding execution thread, at which point it is resumed and provided with the raw performance counter values by the kernel component.

\begin{figure*}
	\centering
	\begin{tikzpicture}[font=\scriptsize]
		\sffamily
		\pgfmathsetmacro{\componentrectwidth}{4}
		\pgfmathsetmacro{\componentrectheight}{4.75}
		\pgfmathsetmacro{\separatordist}{2.75}
		\pgfmathsetmacro{\arrowoverlength}{0.25}
		\pgfmathsetmacro{\arrowlength}{2*(\arrowoverlength + \separatordist)}
		\pgfmathsetmacro{\halfcomponentrectwidth}{\componentrectwidth*0.5}
		\pgfmathsetmacro{\quartercomponentrectwidth}{\componentrectwidth*0.25}

		% kernel
		\draw (0cm,0cm) rectangle ++(\componentrectwidth cm,\componentrectheight) [ref=kernel-rect];
		\node[color=kitblue] at ([yshift=-0.4 cm] kernel-rect south) {Kernel module};
		%\draw[densely dotted] ([yshift=-1cm] kernel-rect north west) -- ++(\componentrectwidth,0) [ref=kernel-sep];
		%\node at ([yshift=-0.5cm] kernel-rect north) {Linux};

		% separator
		\draw[dashed, color=kitdarkgrey] ([xshift=\separatordist cm] kernel-rect south east) -- ([xshift=\separatordist cm] kernel-rect north east) [ref=separator];

		% user-space
		\draw ([xshift=\separatordist cm] separator south east) rectangle ++(\componentrectwidth,\componentrectheight) [ref=user-rect];
		\draw[densely dotted, color=kitdarkgrey] (user-rect south) -- ++(0,\componentrectheight);
		\node[align=center,anchor=north] at ([xshift=-\quartercomponentrectwidth cm] user-rect north) {execution \\ thread};
		\node[align=center,anchor=north] at ([xshift=+\quartercomponentrectwidth cm] user-rect north) {wait \\ thread};
		\node[color=kitblue] at ([yshift=-0.4 cm] user-rect south) {User-space component};

		% steps
		\draw[<-] ([yshift=-0.85cm,xshift=-\arrowoverlength cm] kernel-rect north east) -- ++(\arrowlength,0) node[align=center,pos=.5,above=0] {1. Configuration instructions};
		\node at ([yshift=-1.35cm] kernel-rect north) {2. Setup PMU};
		\draw[->] ([yshift=-2.0cm,xshift=-\arrowoverlength cm] kernel-rect north east) -- ++(\arrowlength,0) node[align=center,pos=.5,above=0] {3. Return to user-space};
		\node[align=center] at ([yshift=-2.5cm,xshift=-\quartercomponentrectwidth cm] user-rect north) {4. Execute \\ AVX};
		\node[color=kityellow] at ([yshift=-3.12cm,xshift=-1cm] kernel-rect north) {\Huge\Lightning};
		\node at ([yshift=-3.1cm] kernel-rect north) {5. Interrupt};
		\draw ([yshift=-3.75cm,xshift=\arrowoverlength cm] user-rect north west) -- ++(-\arrowlength,0) node[align=center,pos=.5,above=0] {6. PMU values};
		\draw[->] ([yshift=-3.75cm,xshift=\arrowoverlength cm] user-rect north west) -- ++(\halfcomponentrectwidth,0);
		\node[align=center] at ([yshift=-4.25cm,xshift=\quartercomponentrectwidth cm] user-rect north) {7. Process \\ results};
	\end{tikzpicture}
	\caption{Simplified analysis framework architecture. The kernel module enables the user-space component to configure the PMU and handles interrupts.}
	\label{fig:analysis:design:overview}
\end{figure*}

\subsection{Kernel Component}
\label{sec:analysis:design:kernel}

Our kernel component is not supposed to conduct any analysis tasks by itself, but is designed to aid the user-space component described later in \Cref{sec:analysis:design:userspace}. We chose to implement it as a module for version~5.1 of the Linux kernel, which implies that it is written in the C programming language. Existence and design of this kernel module are motivated by our user-space component's needs to configure the \gls{PMU} in order to conduct the measurements required for our analysis. This can only be done from kernel-space.

During module startup, the \gls{PMU} is reset to a default state and the performance counter overflow interrupt vector is set in all core's \gls{APIC}'s \glspl{LVT}. Notably, this degrades the functionality of Linux's \texttt{perf} subsystem as \texttt{perf} partially relies on using the \gls{PMU}.

The module interfaces with user-space by defining a device class and then providing a virtual character device of the previously defined class, exposed via \texttt{/dev/reclocking\_analysis} in the virtual file system. User-space may then \texttt{open()} the provided device file and interact with the module by using several offered \texttt{ioctl()} calls.

Execution threads, on the one hand, initiate their execution by using the \texttt{SETUP ioctl()} call. A C \texttt{struct} must be passed that contains a set of \glspl{MSR} to be written by the kernel module -- these are used to configure the \gls{PMU}. In order to increase the precision of our measurements, it is desirable to cut time spent in user-space without actual execution of the code to be measured. Therefore, a value for the \gls{instptr} must also be passed that will be set in the thread's context before returning to user-space, so that the thread will not directly return at the previous position in the \gls{libc}'s \texttt{ioctl()} wrapper but rather be redirected to another location in memory. Optionally, the \texttt{r12}~x86 architectural register may also be set so that the code executed in user-space upon returning is able to access data structures in an easy manner without needing to use the stack. As described further below, the interrupt action must also be defined beforehand by the execution thread. After applying the configuration, the \texttt{ioctl()} handler saves the current \gls{TSC} value and returns to user-space.

Wait threads, one the other hand, start with the \texttt{WAIT\_FOR\_INTERRUPT ioctl()}, which takes a pointer to a \texttt{interrupt\_result} structure in the user-space component where the resulting performance counter values shall be stored later as well as the numeric identifier of a \gls{CPU} core where an interrupt is expected to occur. The calling thread is suspended by setting it into \texttt{TASK\_UNINTERRUPTIBLE} state. This state \cite{kernelschedheader} in Linux's task state machine allows a thread to be woken up only by the kernel itself and not via any user-space mechanisms (e.g., \glspl{UNIXsig}). Consequently, this way we ensure the execution flow is not interrupted unexpectedly.

It is expected that all execution threads that have an associated wait thread trigger a performance counter overflow interrupt some time after setup. The interrupt handler will then proceed with one of multiple actions as instructed by the \texttt{SETUP} call:

\begin{itemize}
	\item \texttt{WAKE\_WAIT\_THREAD}: this action reads all performance counters and writes them along with the current \gls{TSC} value and the recorded \gls{TSC} value at \texttt{SETUP} to the \texttt{interrupt\_result} structure of the corresponding wake wait, which is now waked up from suspension. The execution thread that triggered the interrupt is returned to its previous \acrlong{instptr} (before the \texttt{SETUP} call). Notably, from user-space's view, the original \texttt{SETUP} call returns only now.
	\item \texttt{SET\_MSRS}: this is used for analysis tasks consisting of two consecutive steps. The \gls{TSC} value is recorded and another set of \glspl{MSR} is configured on the thread's core. For the next interrupt, the action is unconditionally set to \texttt{WAKE\_WAIT\_THREAD}.
	\item \texttt{GOTO}: exactly like \texttt{SET\_MSRS}, but also sets a new \acrlong{instptr} on the execution thread.
\end{itemize}

Each of these actions concludes with resetting the \gls{PMU}'s overflow bit and the \gls{APIC}'s state in order to be ready for further interrupts.

A practical software engineering issue arises from the fact that wait threads are suspended in an uninterruptible state after startup: they may easily get stuck due to programming errors that cause a lack of interrupts. For these cases, a third \texttt{ioctl()} call was implemented: \texttt{RESET\_WAIT\_THREADS}, which simply wakes all suspended wait threads and makes their pending \texttt{ioctl()} calls return with an error status.

\subsection{User-Space Component}
\label{sec:analysis:design:userspace}

The user-space component of our analysis framework is, akin to our kernel module, written in C with some additional helper scripts implemented in the PHP scripting language for invocation and monitoring tasks and to generate spreadsheets containing the results. \gls{AVX} instructions included in the program are directly written in x86 assembly.

In order to run tests with different instruction types, an arbitrary number of \gls{ELF} sections containing \gls{AVX} instructions may be included in the component's compiled binary executable. Address within the binary and length of one of these sections must be passed as arguments to the program (these values are easily obtainable using tools like \texttt{objdump}). On startup, one or more (as required by the selected measurement mode) executable memory areas, each consisting of four pages, are mapped and filled with the content of the passed section, repeated until the area is full, or alternatively, again depending on the measurement mode, filled only with a specific amount of repeated instructions. In order to ensure the instruction flow does not run outside the allocated area, one of two different loop modes is used at the end of each memory area, depending on the measurement mode:

\begin{itemize}
	\item \texttt{LOOP\_AVX}: a jump instruction to the beginning of the area is added in order to make the constructed code loop.
	\item \texttt{LOOP\_R12\_CMP}: a spinlock-style loop is inserted that constantly compares the value referenced by the pointer stored in the \texttt{r12} register to $0$ and returns as soon as it isn't equal to $0$ anymore:
		\begin{minted}{gas}
		loop:
		CMP 0x0, (%r12)
		JE loop
		RET
		\end{minted}
		Note that our \gls{AVX} memory area does not have its own stack frame in any way, so we actually return the \texttt{SETUP ioctl()}'s stack frame here. This is a rather fragile and non-portable approach and may not work as desired with every \gls{libc} implementation.
\end{itemize}

The number of four pages was not chosen arbitrarily: on an x86 processor running in \SI{64}{\bit} mode \cite{intelsdmsysprogguide}, pages have a default size of \SI{4}{\kibi\byte}, thus four pages equate a total size of \SI{16}{\kibi\byte}. We originally used an area size of \SI{2}{\mebi\byte} (512 pages), based on the idea of achieving a purely homogeneous workload that does not contain any jumps to increase the precision of our measurements. However, tests showed that the code would become approximately \SI{20}{\percent} faster when run on multiple cores in parallel. We believe this behavior to be caused by instruction cache misses -- although modern \glspl{CPU} commonly feature an \gls{instprefetcher}, there is a caveat: it does not load instructions across page boundaries, and thus, every \SI{4}{\kibi\byte} of instructions we would see a cache miss and a costly pipeline stall until the next instructions arrive from memory. By using parallel execution on multiple cores, this effect is mitigated as the fastest core would already have loaded the instructions into the \gls{L3}, readily available for the other cores. It could be worthwhile to look into huge pages as an alternative solution, though that remains future work.
% TODO is this related to spectre?

As briefly described in \Cref{sec:analysis:design:overview}, the user-space process spawns $n~\in~\mathbb{N}$ execution threads and $w\in\{1,n\}$ wait threads. The value for $n$ is specified as a command line argument, while the amount of wait threads depends on whether \textit{pre-throttling} is enabled. The execution threads are bound to \gls{CPU} cores $1$ to $n$, respectively; the wait threads to the following cores. This also implies that at maximum $\lfloor{}\frac{C}{2}\rfloor{}$ execution threads may be run, where $C$ is the number of \gls{CPU} cores installed in the system, and a minimum of two cores must be available. In pre-throttling mode, the idea is to create an artificial, pre-existing global load situation across several cores. Only results from one core are collected, and thus, only one wait thread is required. The other execution threads start their execution about \SI{150}{\milli\second} earlier than the monitored thread and execute either an infinite (scalar) loop or the same \gls{AVX} code that will also run on the monitored thread, depending on the selection made via the command line. With pre-throttling enabled, it is theoretically possible to have $C-1$ execution threads running.

In order to avoid inaccuracies caused by preemption, all execution threads use the \texttt{SCHED\_RR} scheduling policy offered by Linux's \gls{CFS} \cite{cfs} which is designed for near-real-time execution and selects real-time threads ordered by their priority; threads of equal priority are executed in a round-robin fashion. \gls{CFS} exposes additional configuration settings \cite{cfsrt} to control the fraction of time that may be consumed by real-time processes, namely \texttt{sched\_rt\_period\_us} and \texttt{sched\_rt\_runtime\_us}. The former sets a time window (\SI{1}{\second} per default) and the latter contains the absolute amount of time within that window that is available to real-time threads (\SI{950}{\milli\second} per default). In theory, we could configure these to allow for infinite real-time execution, however, practical tests have shown this leads to unbearable system hangs that would require further work on our implementation in order to fix. We settled for a value of \SI{990}{\milli\second} for \texttt{sched\_rt\_runtime\_us} as compromise.

In all measurement modes, we configure the following performance events (as documented by Intel in \cite{intelsdmsysprogguide}):

\begin{itemize}
	\item \texttt{CORE\_POWER.LVL0\_TURBO\_LICENSE}: counts core cycles spent in turbo license level 0.
	\item \texttt{CORE\_POWER.LVL1\_TURBO\_LICENSE}: counts core cycles spent in turbo license level 1.
	\item \texttt{CORE\_POWER.LVL2\_TURBO\_LICENSE}: counts core cycles spent in turbo license level 2.
	\item \texttt{CORE\_POWER.THROTTLE}: counts core cycles during which the \gls{OoO} engine is throttled.
	\item \texttt{INT\_MISC.CLEAR\_RESTEER\_CYCLES}: counts core cycles while the execution engine is stalled waiting for instructions to be delivered -- this is used to estimate the time spent before the actual execution when switching from kernel-space to user-space in execution threads.
	\item \texttt{FP\_ARITH\_INST\_RETIRED.PACKED}: counts retired packed floating-point \glspl{vectorinst}. Several variants for \SI{128}{\bit}, \SI{256}{\bit} and \SI{512}{bit} vectors and single- and double-precision instructions are available which we select according to the instruction type used in the \gls{AVX} code section passed at startup.
	\item \texttt{UOPS\_DISPATCHED\_PORT.PORT\_0}: counts \glspl{microinst} dispatched by the processor's scheduler at execution port 0. The use of this performance event is motivated by the \textit{Skylake (Server)} microarchitecture the \gls{CPU} we used for our analysis is based on. These processors have \cite{intelxeonscalabledeepdive} an \gls{AVX-512} unit fused from two \SI{256}{\bit} units at ports 0 and 1. For other microarchitectures, other performance events may be appropriate.
	\item \texttt{UOPS\_DISPATCHED\_PORT.PORT\_5}: counts \glspl{microinst} dispatched by the processor's scheduler at execution port 5. The motivation here is the same as for the performance event counting \glspl{microinst} at port 0, however, only some specific \textit{Skylake (Server)} \glspl{CPU} have an additional, dedicated (i.e., non-fused) \gls{AVX-512} unit at port 5.
\end{itemize}

At startup, wait threads simply block at a synchronization barrier after setting their core affinity. Execution threads, in contrast, need to set their core affinity, the scheduling policy and build up the configuration to pass to the \texttt{SETUP ioctl()} call provided by our kernel module. Afterwards, they block at the same synchronization barrier as the wait threads, unless the thread is used for pre-throttling as described above. As soon as all threads have reached the barrier, the execution threads will enter a \SI{150}{\milli\second} busy-wait loop before calling the \texttt{SETUP ioctl()} to ensure their respective cores are ramped up to their maximum turbo frequency before starting the test run, whereas the wait threads directly jump into the \texttt{WAIT\_FOR\_INTERRUPT ioctl()}.

After execution has completed, there is not much to do for the execution threads: their \texttt{SETUP ioctl()} returns and then they simply exit. Wait threads, on the other side, will again synchronize at a barrier and then output the results as provided by the kernel module one after another before they exit, too. As soon as all threads have completed, the program quits.

\subsection{Measurement Modes}
\label{sec:analysis:design:measurementmodes}

In order to answer the questions named in \Cref{sec:analysis:methodology}, we implemented several different measurement modes in our user-space component, which are to be presented hereafter.

\subsubsection{DOWNCLOCK}
\label{sec:analysis:design:measurementmodes:downclock}

Our first measurement mode is designed to measure the downclocking behavior~--~i.e., how long it takes for a \gls{CPU} to reduce its frequency and whether there is a step to turbo license level 1 before switching to level 2 for instructions that target level 2.

In this mode, we simply map a single memory area with \gls{AVX} code to be run by all execution threads and configure the \gls{PMU} to trigger an interrupt and freeze the performance counters as soon as one cycle is spent in either level 1 or level 2, depending on the target license level passed as command line argument to the program. We use the \texttt{WAKE\_WAIT\_THREAD} interrupt action provided by the kernel component. Thereby, we can measure the time taken for the frequency reduction. When running a test case using this measurement mode with level 2 as target, we will also see whether any cycles were spent in level 1 from the respective performance counter. Thus, this measurement mode indeed answers the aforementioned questions.

\subsubsection{UPCLOCK}
\label{sec:analysis:design:measurementmodes:upclock}

After analyzing the downclocking times, the next logical step is to look at the reverse process: the upclocking. Here, we are mainly interested in the time the \gls{CPU} takes before returning back to its non-throttled frequency after the last \gls{AVX} instruction has retired.

Like in the \hyperref[sec:analysis:design:measurementmodes:downclock]{\texttt{DOWNCLOCK}} mode, we map an \gls{AVX} memory area where the execution threads jump to on after startup, however, we also map an additional page with an infinite loop. In the first step, we configure an interrupt to be fired after switching to either level 1 or level 2, depending on the input, in order to be able to measure upclocking from both throttle levels. Then, using the \texttt{GOTO} interrupt action, we move the execution thread to the infinite loop page, reset our performance counters and configure an interrupt for when level 0 is reached again and to freeze the performance counters at this point. It is important to not simply leave the core in a completely idle state, as the kernel would then eventually run the \texttt{MWAIT} \cite{intelsdminstructionreference} instruction on the core, causing it to enter a halt state, and thus, our measurements would be useless. Using the described procedure, we measure only the time spent after reaching a turbo license level with reduced frequency until returning to nominal frequency, which is exactly what we are interested in. Notably, in this mode, we instruct the \gls{PMU} to also count cycles while running in kernel-space (i.e., ring~$0$) as this is also time spent without executing \gls{AVX} instructions, and thus, must be measured to retrieve precise results.

\subsubsection{PRE\_THROTTLE\_THROUGHPUT}
\label{sec:analysis:design:measurementmodes:prethrottlethroughput}

As cited at the beginning of \hyperref[sec:analysis]{this chapter}, Intel talks in their optimization manual \cite{inteloptimizationmanual} about the \glspl{CPU} operating at a \enquote{lower peak capability} before the switch to a turbo license level with lower frequency is completed. Early experimentation showed this state is seemingly represented by the \texttt{CORE\_POWER.THROTTLE} performance event which is described \cite{intelsdmsysprogguide} to count cycles where the \gls{OoO} engine is throttled. We want to find out when exactly this throttle state is activated and what instruction throughput the \gls{CPU} achieves before throttling to get an idea of the theoretically possible performance if the frequency reduction did not exist.

For this purpose, the \texttt{PRE\_THROTTLE\_THROUGHPUT} mode conceptually works very much the same way as the \hyperref[sec:analysis:design:measurementmodes:downclock]{\texttt{DOWNCLOCK}} mode: we configure an interrupt that fires and freezes the performance counters as soon as the first cycle was spent in throttled mode and run our \gls{AVX} code using the \texttt{WAIT\_FOR\_INTERRUPT} interrupt action. Thereby, we obtain the desired information about the behavior during the time between starting execution and \gls{OoO} engine throttling.

\subsubsection{NON\_AVX\_TIME}
\label{sec:analysis:design:measurementmodes:nonavxtime}

To obtain a model of the reclocking algorithm that is as complete as possible, we are not only interested in the time it takes for a \gls{CPU} to switch turbo license levels, but also how many instructions are precisely required to eventually trigger the frequency reduction.

The implementation of this measurement mode is somewhat more complex compared to the other modes and also slightly depends on the license level transition to be examined. In every case, we map an \gls{AVX} memory area in \texttt{LOOP\_R12\_CMP} mode which initially contains only just one copy of the \gls{AVX} code in the selected \gls{ELF} section. When license level 2 was chosen as target, we additionally map an area in \texttt{LOOP\_AVX} mode that is executed until level 1 is reached. In \gls{AVX} pre-throttling mode, another area is created in \texttt{LOOP\_AVX} mode to be run by execution threads used for pre-throttling.

In case level 1 is targeted, all (non-pre-throttling) execution threads directly jump into the \texttt{LOOP\_R12\_CMP} area and use the \texttt{WAKE\_WAIT\_THREAD} interrupt action mode. For level 2 as target, we select the \texttt{GOTO} interrupt action and first jump into the aforementioned \texttt{LOOP\_AVX} area and configure an interrupt to be triggered as soon as one cycle in level 1 was completed. Afterwards, the execution threads are also moved to the \texttt{LOOP\_R12\_CMP}. In our \texttt{SETUP} configuration, we instruct our kernel module to set the value of the \texttt{r12} register to a pointer to a global \texttt{try\_elapsed} variable defined in the program so that the loop in the executed code returns as soon as \texttt{try\_elapsed} is set.

While the execution threads are running, our main thread awaits a \SI{1}{\milli\second} delay and then checks whether an interrupt was triggered on all expected cores. If yes, the test run has completed and the program quits. Otherwise, we exit all execution threads by setting \texttt{try\_elapsed} and then remap the \texttt{LOOP\_R12\_CMP} with one more copy of the \gls{AVX} code than in the previous iteration. Wait threads that completed because an interrupt was triggered on their corresponding execution thread are respawned, afterwards we reset \texttt{try\_elapsed} and start the execution threads again. Unlike in the first iteration, where execution threads generally spin for \SI{150}{\milli\second} to ramp up the core frequency (as described in \Cref{sec:analysis:design:userspace}), we only have them spin for \SI{3}{\milli\second} now as their respective cores are already running at the desired frequency but possibly need to return from an attained turbo license level. This procedure repeats until we have enough \gls{AVX} code in our mapped area to trigger interrupts on all desired cores.

At the end, the number of copies of the \gls{AVX} code in the \texttt{LOOP\_R12\_CMP} area reflects the amount of instructions needed to cause the frequency transition.

\section{Results}

We use the described analysis framework to conduct measurements with several different \gls{AVX} instructions using all available combinations of modes, i.e., with and without pre-throttling, different target turbo license levels, with turbo frequencies enabled and disabled and with all possible amounts of cores enabled. In this section, we present our system setup, describe the precise instructions we used for testing and present as well as discuss the results.

\subsection{System Setup}

To execute our analysis tasks, we used an Intel Core i9-7940X processor which features 14 physical cores with twofold \gls{SMT} running at a nominal base frequency of \SI{3.1}{\giga\hertz} with a maximum turbo frequency of \SI{4.3}{\giga\hertz} \cite{intel7940x}. Additionally, the chip supports \textit{Intel Turbo Boost Max Technology 3.0}, essentially meaning that two cores may operate at a higher turbo frequency, namely \SI{4.4}{\giga\hertz}. These cores are selected based on their electrical and thermal properties during the manufacturing process \cite{intelxeonscalabledeepdive} -- this technique is otherwise also known as \textit{speed binning} \cite{lopata2012speed}. The chip's nominal \gls{TDP} is specified at \SI{165}{\watt}, and as such, this is the maximum power consumption the chip will sustain over long time periods.

This \gls{CPU} is based on the \textit{Skylake (Server)} microarchitecture, the first x86 implementation featuring support for the \gls{AVX-512} extension \cite{intelxeonscalabledeepdive}, making it one of the oldest processors that expose the \gls{AVX} reclocking issue for heterogeneous workloads. However, not all \glspl{CPU} built with this microarchitecture feature the same amount of \SI{512}{\bit}-vector execution units: some have two, others only one. The i9-7940X used here has two.

The processor was equipped on an ASUS TUF X299 MARK 2 motherboard along with \SI{32}{\gibi\byte} of DDR4 system memory operating at a frequency of \SI{2666}{\mega\hertz} and a \gls{NVMe} Solid State Drive.

We opted to use Fedora 29 (Server Edition) as operating system with a custom-built Linux 5.1.0 kernel and glibc version 2.28-33. The kernel and all of our own code were compiled using GCC 8.3.1-2 with the default \texttt{-O2} optimization level.

In order to minimize overhead and latencies caused by context switches from user-space to kernel-space and vice versa, we disabled all mitigations provided by the Linux kernel for hardware vulnerabilities found in recent \glspl{CPU} (e.g., \textsc{Spectre} and \textsc{Meltdown}) as well as \gls{KASLR}.

\subsection{Tested Instructions}

In order to create a precise model of the \gls{AVX} reclocking algorithm as it was implemented by Intel, we want to conduct our measurements with different kinds of \gls{AVX} instructions to find possible differences in the behavior. Note that we only tested homogeneous loads and did not run any tests with heterogeneous mixtures of different instruction classes. This remains as future work.

We tried to select both floating-point and integer operations that reflect the \enquote{heavy} and \enquote{light} instruction types as defined in Intel's optimization manual \cite{inteloptimizationmanual} as well as instructions we guess to be implemented differently in the hardware's execution units. Consequently, we chose the following subset of \gls{AVX} instructions for our measurements (as obtained from Intel's manual for software developers \cite{intelsdminstructionreference}):

\begin{itemize}
	\item \texttt{vfmaddsub132pd} (double-precision) and \texttt{vfmaddsub132ps} (single-precision): these are floating-point \gls{FMA} instructions that alternatingly add and subtract the values from a third vector after multiplying the values from two other vectors. I.e., for input vectors $a$, $b$ and $c$, they calculate the result vector $r$ according to the following rule:
	\begin{displaymath}
		\begin{pmatrix}
		r_1 \\
		r_2 \\
		r_3 \\
		\dots
		\end{pmatrix}
		\coloneqq
		\begin{pmatrix}
		a_1 * b_1 + c_1 \\
		a_2 * b_2 - c_2 \\
		a_3 * b_3 + c_3 \\
		\dots
		\end{pmatrix}
	\end{displaymath}
	\item \texttt{vmulpd} (double-precision) and \texttt{vmulps} (single-precision): those instructions simply calculate the products of all corresponding floating-point members from two input vectors.
	\item \texttt{vpmullq}: multiplies corresponding \SI{64}{\bit} integers from two input vectors into \SI{128}{\bit} intermediate results and stores the lower \SI{64}{\bit}s of each in the target result vector.
	\item \texttt{vpackssdw}: this instruction merges two vectors with signed \SI{32}{\bit} integers into one vector consisting of signed \SI{16}{\bit} integers by handling overflow conditions via saturation arithmetic, i.e., for values larger than $32767$ ($=2^{15}-1$) or smaller than $-32768$ ($=-2^{15}$), the conversion results in these extreme values. In mathematical terms, the operation may be described as follows for input vectors $a$, $b$ and result vector $r$:
		\begin{displaymath}
		\forall i \in \{1,\dots,|a|\}\colon r_i \coloneqq saturate(a_i), r_{|a|+i} \coloneqq saturate(b_i)
		\end{displaymath}
		where $saturate$ is defined as
		\begin{displaymath}
		saturate\colon
		\begin{cases}
			\{-2^{31}, \dots, 2^{31}-1\} \cap \mathbb{Z} \longrightarrow \{-2^{15}, \dots, 2^{15}-1\} \cap \mathbb{Z}, \\
			x \mapsto min(2^{15}-1, max(x, -2^{15})).
		\end{cases}
		\end{displaymath}
		Note that $|a| = |b|$ and $|r| = |a| + |b|$.
	\item \texttt{vpaddsw}: adds signed \SI{16}{\bit} integers from two input vectors using saturation arithmetic as described above.
	\item \texttt{vpmaddwd}: an \gls{FMA}-style operation that first multiplies corresponding signed \SI{16}{\bit} integers from two input vectors, thereby creating an equal amount of \SI{32}{\bit} temporary results. Afterwards, the adjacent results are added together to generate the result vector. For input vectors $a$ and $b$, this is the operation executed to obtain the result vector $r$:
		\begin{displaymath}
		\begin{pmatrix}
		r_1 \\
		r_2 \\
		\dots
		\end{pmatrix}
		\coloneqq
		\begin{pmatrix}
		(a_1*b_1) + (a_2*b_2) \\
		(a_3*b_3) + (a_4*b_4) \\
		\dots
		\end{pmatrix}
		\end{displaymath}
\end{itemize}

We wrote \gls{ELF} sections for our user-space component (as described in \Cref{sec:analysis:design:userspace}) containing assembly code for all of these instructions in two variants with \SI{256}{\bit} \texttt{YMM} and \SI{512}{\bit} \texttt{ZMM} registers, respectively. Additionally, for each variant, there are two versions: an \enquote{unrolled} one and another non-\enquote{unrolled} version. The non-unrolled ones simply contain a single instruction using the first three registers, e.g.:

\begin{center}
	\begin{minted}{gas}
	vfmaddsub132pd %zmm0, %zmm1, %zmm2
	\end{minted}
\end{center}

By constantly keeping to execute the same instruction with the same operands, we create some artificial register pressure that prevents a core's scheduler from maximizing utilization of the two \SI{512}{\bit}-vector units available in the execution engine. The unrolled versions, on the other side, alleviate this pressure by repeating the same instruction, but with different register operands:

\begin{center}
	\begin{minted}{gas}
	vfmaddsub132pd %zmm0, %zmm0, %zmm1
	vfmaddsub132pd %zmm0, %zmm0, %zmm2
	vfmaddsub132pd %zmm0, %zmm0, %zmm3
	...
	\end{minted}
\end{center}

Every unrolled section contains the same instruction repeated 31 times, always using \texttt{\%zmm0}/\texttt{\%ymm0} for the first two operands and \texttt{\%zmm\{1-31\}}/\texttt{\%ymm\{1-31\}} as last operand, thereby, we exhaustively make use of all 32 \texttt{ZMM}/\texttt{YMM} architectural registers available on \gls{AVX-512}-capable processors.