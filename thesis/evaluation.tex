\chapter{Evaluation}
\label{sec:evaluation}

We have presented the design of \textsc{avxfreq}, a software-based reimplementation of a partial model of Intel's \gls{AVX} reclocking algorithm, in the previous chapter. Further, we have described how \textsc{avxfreq} can be leveraged to allow user-space programs to choose the applied turbo license levels themselves. In this chapter, we will evaluate these implementations: in the case of \textsc{avxfreq}, we will present modifications to \textsc{avxfreq} itself as well as the analysis framework previously presented in \Cref{sec:analysis} that allow us to use the latter in order to compare \textsc{avxfreq}'s behavior to Intel's hardware implementation and the simplified algorithm we wanted to reflect. For the user-space-driven decisions we will describe the design of a simple program that simulates heterogeneous workloads with both scalar and \gls{AVX} instructions. Finally, we will present the results obtained from executing tests with these tools.

\section{Design}
\label{sec:evaluation:design}

% TODO text here

\subsection{\textsc{avxfreq}}
\label{sec:evaluation:design:avxfreq}

To be able to evaluate the quality of \textsc{avxfreq}'s implementation and to measure how good it reflects the hardware's reclocking behavior, it seems a good idea to make use of the analysis framework from \Cref{sec:analysis} -- in the best case, running it with \textsc{avxfreq} should deliver similar results to what is seen with hardware reclocking. However, our framework relies on using the processor's \gls{PMU} (as described in \Cref{sec:analysis:design:pmu}), just like \textsc{avxfreq} does, and, due to the limited amount of available performance counters they may not use the \gls{PMU} concurrently. Further, hardware-side performance events that count cycles a processor's core spends in a specific turbo license level do not make sense anymore when \textsc{avxfreq} is enabled as the license levels are now emulated by software.

To circumvent these obstacles, we adapted \textsc{avxfreq} and our analysis framework to work together to emulate the most important performance events in software: the idea is that \textsc{avxfreq} already uses some of the performance events that the analysis system would need anyway and that it is perfectly capable of counting cycles spent in different states. Therefore, we let \textsc{avxfreq} do the bookkeeping and provide our framework's kernel component with software-generated events instead of interrupts in a manner that is completely transparent to the user-space part.

\textsc{avxfreq} in itself only needs one performance counter to count retired \SI{512}{\bit} floating-point double-precision packed vector operations and, apart from that, also makes use of a fixed counter to count core cycles. Thus, we need to provide these to the analysis framework's kernel module -- other counters may be managed by the module itself. For this purpose, \textsc{avxfreq} keeps raw counters in software that are updated using the values provided by the \gls{PMU} whenever any event triggers \textsc{avxfreq} code to be run. To enable interaction between the analysis framework's kernel module and \textsc{avxfreq}, we defined and implemented the following kernel-internal interface:

\begin{itemize}
	\item \mintinline{c}{bool avxfreq_is_enabled(void)} \\
		Returns \mintinline{c}{true} if \textsc{avxfreq} was enabled during system boot, \mintinline{c}{false} otherwise. If \textsc{avxfreq} is not enabled, there is no reason for the analysis framework to do anything differently.
	\item \mintinline{c}{avxfreq_counters *avxfreq_get_counters(int cpu)} \\
		\mintinline{c}{avxfreq_counters} is a C language \mintinline{c}{struct} that contains the raw counters for the values defined above. One instance is defined for every core in the system. Given a core number, this method returns a pointer to the instance for the respective core.
	\item \mintinline{c}{void avxfreq_reset_cycle_counter(void)} \\
		This method resets the current core's cycle fixed counter. % TODO why the fuck do we need this
	\item \begin{minted}[autogobble]{c}
			void avxfreq_set_license_transition_listener
			     (avxfreq_license_transition_listener listener)
		\end{minted}
		\vspace{-0.45cm} % TODO what's the correct value here?
		Using this method, the analysis kernel module can hook into \textsc{avxfreq} and receive notifications whenever the applied virtual license level changes. The argument provided is a pointer to a function that takes two arguments: the previous virtual license level and the new one.
\end{itemize}

Using this interface, we modify our kernel component as follows: when loaded, it calls \texttt{avxfreq\_is\_enabled()} to check whether \textsc{avxfreq} is active. If so, it will not reconfigure the \gls{APIC} to take over handling of \gls{PMU} interrupts, but rather call \texttt{avxfreq\_set\_license\_transition\_listener()} to receive updates from \textsc{avxfreq} about license level switches and \texttt{avxfreq\_get\_counters()} once for every \gls{CPU} core in the system to locally store pointers to all counters provided by \textsc{avxfreq}. In addition to the raw counters, we added a local structure that contains derived counters, e.g., \textsc{avxfreq} only counts core cycles since they were last reset, but we also need to be able to distinguish cycles spent in the different license levels.

Whenever the module needs to write to \gls{PMU} \glspl{MSR} after being instructed to do so by the user-space component (e.g., during the \texttt{SETUP ioctl()}), it stores updated performance counter configurations in an array, so that it can later map them to software counters from \textsc{avxfreq}. Direct writes to performance counters are remapped to the corresponding software counters, unless there is no software counter available for them -- in this case, they are routed to the \glspl{MSR} just like when \textsc{avxfreq} is disabled. Similar action is taken when trying to read performance counters. This way, \gls{PMU} configuration remains fully transparent to user-space and hardware counters are automatically remapped to software counters when required without need for further intervention.

Interrupt handling is, as previously mentioned, disabled in case \textsc{avxfreq} is enabled. Instead, when the \texttt{avxfreq\_license\_transition\_listener} is called, it will update all local software counters with the values currently set in \textsc{avxfreq}'s counters and trigger the interrupt handler if the license level transition reflects one for which user-space requested an interrupt. Again, using this mechanism, we substitute hardware interrupts with software-based events in a manner that is transparent to user-space.

The interrupt handler itself (which now does not necessarily handle \emph{interrupts}) only needs a very small modification to work in this scenario: it must not reset the interrupt state in hardware, both for the \gls{PMU} and the \gls{APIC}. Apart from that, all interrupt actions we defined in \Cref{sec:analysis:design:kernel} work the same way as before.

As shown, these modifications allow us to fully employ our analysis framework with all its features and without drawbacks in combination with \textsc{avxfreq} enabled.

\subsection{Staged Execution}
\label{sec:evaluation:design:stagedexecution}

Given that our work is originally motivated by the issue of performance degradation when executing heterogeneous workloads that make use of both \gls{AVX} and scalar instructions, it makes sense to have a tool that allows us to simulate such workloads. Together with the results from our analysis in \Cref{sec:analysis}, such a tool would allow us to create worst-case scenarios that we can use to measure the maximum negative impact of heterogeneous workloads. Further, we can use it to evaluate any improvements achieved with modified reclocking algorithms, e.g., the user-space-driven decisions we presented in \Cref{sec:design:userspacedrivendecisions}.

The idea is to implement a tool that executes multiple \emph{stages} of arbitrary duration, where each stage represents a different load type: scalar, \gls{AVX} instructions targeting turbo license level~1, and \gls{AVX} targeting level~2. We use \SI{256}{\bit} and \SI{512}{\bit} \gls{FMA} instructions (\texttt{vfmaddsub132pd}, precisely) for the two latter stage types, respectively, whereas scalar stages only use simple increment and jump instructions. This was not an arbitrary choice: \textsc{avxfreq}, by design, only tries to resemble the hardware's implementation for \SI{512}{\bit} \gls{FMA} instructions.

The user may pass an arbitrary amount of arguments to the program, each representing a stage and how long it should be executed in \si{\micro\second}. For example, the first argument represents a run of a scalar stage, the second one a level~1 stage, the third one a level~2 stage, the fourth one a scalar stage again, and so on. Our implementation then runs each of the passed stages one after another for the given duration. During startup, the program spawns a separate thread and binds it to a specific core to ensure the process scheduler does not migrate the thread between cores and enables the \texttt{SCHED\_RR} scheduling policy to engange real-time execution -- just like our analysis framework's user-space component (see \Cref{sec:analysis:design:userspace}). This thread then executes each stage by running a loop where each iteration contains a single instruction corresponding to the stage type as described above. We enforce the configured time windows for each stage by having the main thread sleep for this duration and notify the executing thread after it has passed. Performance is then measured by counting how many iterations of the loop are executed within the stage's time window.

Additionally, in order to implement the user-space-driven decisions as mentioned above, we have added a command line option which makes use of the interface we presented in \Cref{sec:design:userspacedrivendecisions} to set license levels according to the stage that is to be executed.

\begin{figure*}
	\centering
	\begin{tikzpicture}[font=\scriptsize]
		\sffamily
		\pgfmathsetmacro{\linelength}{\textwidth-3cm}
		\begin{axis}[
			scale only axis,
			width=\linelength,
			height=1cm,
			xlabel={\si{\micro\second}},
			axis y line=none,
			axis x line=bottom,
			xmin=0,
			xmax=1000,
		]
			\addplot[only marks, color=white] coordinates {(0,0) (1000,0)}; % this is necessary because, well, i don't know
		\end{axis}

		\pgfmathsetmacro{\barheight}{0.2}
		\pgfmathsetmacro{\linemargin}{0.8}
		\pgfmathsetmacro{\linepadding}{0.1}
		\pgfmathsetmacro{\linedistance}{0.7}
		\pgfmathsetmacro{\linezeroy}{2.1}
		\pgfmathsetmacro{\lineoney}{\linezeroy-\linedistance}
		\pgfmathsetmacro{\linetwoy}{\lineoney-\linedistance}
		\pgfmathsetmacro{\levelheight}{0.2}

		% level 0
		\node[color=kitblue, minimum height=\barheight cm, text centered] at (-\linemargin, \linezeroy) {\SI{512}{\bit} AVX};
		\draw[color=kitdarkgrey] (0, \linezeroy) -- ++(\linelength pt, 0);

		% level 1
		\node[color=kitblue, minimum height=\barheight cm, text centered] at (-\linemargin, \lineoney) {\SI{256}{\bit} AVX};
		\draw[color=kitdarkgrey] (0, \lineoney) -- ++(\linelength pt, 0);

		% level 2
		\node[color=kitblue, minimum height=\barheight cm, text centered] at (-\linemargin, \linetwoy) {Scalar};
		\draw[color=kitdarkgrey] (0, \linetwoy) -- ++(\linelength pt, 0);

		\pgfmathsetmacro{\baronelength}{\linelength*0.2}
		\pgfmathsetmacro{\bartwolength}{\linelength*0.15}
		\pgfmathsetmacro{\barthreelength}{\linelength*0.4}
		\pgfmathsetmacro{\barfourlength}{\linelength*0.1}
		\pgfmathsetmacro{\bartwox}{0+\baronelength}
		\pgfmathsetmacro{\barthreex}{\bartwox+\bartwolength}
		\pgfmathsetmacro{\barfourx}{\barthreex+\barthreelength}
		\draw[fill=kitgreen, color=kitgreen] (0, 2.0) rectangle ++(\baronelength pt, \levelheight cm);
		\draw[fill=kitgreen, color=kitgreen] (\bartwox pt, 1.3) rectangle ++(\bartwolength pt, \levelheight cm);
		\draw[fill=kitgreen, color=kitgreen] (\barthreex pt, 0.6) rectangle ++(\barthreelength pt, \levelheight cm);
		\draw[fill=kitgreen, color=kitgreen] (\barfourx pt, 1.3) rectangle ++(\barfourlength pt, \levelheight cm);
	\end{tikzpicture}
	\caption{Exemplary run of our staged execution tool when called with \texttt{200~150~400~0~100} as command line. \SI{512}{\bit} \gls{AVX} is executed for a duration of \SI{200}{\micro\second}, then \SI{256}{\bit} \gls{AVX} for \SI{150}{\micro\second}, then purely scalar instructions for \SI{400}{\micro\second}, and finally a last stage with \SI{256}{\bit} \gls{AVX} for another \SI{100}{\micro\second}.}
	\label{fig:evaluation:design:stagedexecution}
\end{figure*}

\section{Results}
\label{sec:evaluation:results}

For our evaluation we used the same system that we previously described in \Cref{sec:analysis:results:systemsetup}. However, to make \textsc{avxfreq} work correctly, we needed to modify some settings in the system's \gls{UEFI} configuration: first, we had to disable Intel \gls{HWP} as the firmware enables it by default, whereas it is necessary to have \gls{HWP} disabled for \textsc{avxfreq} to allow it to exercise full control over the processor's \glspl{P-state} as outlined in \Cref{sec:design:reimplementation:intel_pstate}. Second, while it is not possible to disable \gls{AVX}-induced reclocking entirely, we have set the reclocking offsets to 1 for both license levels to minimize the impact by the hardware's implementation -- as pointed out in our description of the system's hardware, this is the reason we chose that specific motherboard. This, though, also means that with \textsc{avxfreq} enabled, there are two reclocking responses to the execution of \gls{AVX} code: by the processor itself and by \textsc{avxfreq}. While the resulting final frequencies still equal the ones we try to reach, frequency changes potentially happen twice.

In this section, we will present the results obtained by executing tests using the modified \textsc{avxfreq} and analysis framework implementations as well as the staged execution tool we presented in the previous section.

\subsection{\textsc{avxfreq}}
\label{sec:evaluation:results:avxfreq}

As discussed in \Cref{sec:analysis:design:measurementmodes}, our analysis framework provides multiple measurement modes: \texttt{DOWNCLOCK} to measure the time taken until a frequency reduction is completed, \texttt{UPCLOCK} for the reverse process, \texttt{PRE\_THROTTLE\_THROUGHPUT} to determine the time before a core is throttled after \gls{AVX} execution has begun, and \texttt{NON\_AVX\_TIME} to find out how many instructions exactly are required to trigger the reclocking. To verify how well \textsc{avxfreq} implements the simplified reclocking algorithm we wanted to reflect, we executed tests using all these modes with \textsc{avxfreq} and the above-mentioned system configuration, with the exception of the \texttt{PRE\_THROTTLE\_THROUGHPUT} mode: as the throttling is purely controlled by the hardware itself and not affected by any system configuration or \textsc{avxfreq}, it makes no sense to run this mode again here. Further, \textsc{avxfreq} is designed to only resemble the \gls{CPU}'s behavior when executing \SI{512}{\bit} \gls{FMA} instructions, with turbo frequencies enabled, and with only one active core, and as such, we only ran tests under these specific conditions.

In the case of transitions from level~0 to level~1 measured using the \texttt{DOWNCLOCK} and \texttt{NON\_AVX\_TIME} modes, there are no surprises: \textsc{avxfreq} was designed to immediately switch to level~1 as soon as a single \SI{512}{\bit} double-precision floating point vector instruction is retired and we find this to essentially hold true. The amount of completed operations in the \texttt{DOWNCLOCK} test for the unrolled case (i.e., when both \gls{AVX-512} units of the core are utilized in parallel) varies between 18 and 34 (equaling 9 and 17 instructions, respectively, as a \gls{FMA} instruction consists of two operations) before \textsc{avxfreq} invokes and reports the transition to level~1. While there is a small deviation here, we believe this is caused by \gls{PMU} interrupts being slightly delayed and consider it to be negligible. This assumption is supported by the fact that the \texttt{NON\_AVX\_TIME} test generally shows one single instruction to be sufficient to trigger a level transition.

Transitions to level~2 examined using the same measurement modes, however, show a larger deviation between the model we tried to implement and the actual results: the median of the time needed here lies at \SI{103.8}{\micro\second}. Looking at how \textsc{avxfreq} was implemented, though, this is not surprising: after having moved to level~1, \textsc{avxfreq} will trigger interrupts every \SI{100}{\micro\second} to check whether any of the measured instructions were executed in meantime and then either triggers a switch to level~2 if the throughput is high enough or counts towards the upclocking timer if no instructions were executed. Thus, a transition from level~1 to level~2 may only occur at least \SI{100}{\micro\second} after the transition from level~0 to level~1. A plausible explanation for the remaining $\sim$\SI{4}{\micro\second} could be delays caused through context switches and kernel code. % TODO future work => smaller first interval - name here or later?

Tests using the \texttt{UPCLOCK} mode show that \textsc{avxfreq} behaves as designed here: the average and median times taken for reverting a frequency reduction lie at about \SI{665}{\micro\second}, which is slightly below the \SI{666}{\micro\second} we were aiming at. All results are distributed very homogeneously around this value. Again, we attribute the small deviation to time taken by context switches and code executed within the kernel.

\subsection{Staged Execution}
\label{sec:evaluation:results:stagedexecution}

We designed our staged execution tool with the goal of being able to determine a baseline for what optimized reclocking algorithms could achieve in the best case by using user-space-driven decisions as we described in \Cref{sec:design:userspacedrivendecisions}. Further, this tool allows us additional comparisons between \textsc{avxfreq} and Intel's hardware implementation -- in the previous section, we essentially measured how well our reimplementation reflects the model we wanted to achieve, whereas here, we can verify that \textsc{avxfreq} provides similar performance to the hardware implementation.

To obtain the following results, we ran our staged execution tool using\\\texttt{0~0~2000000~200000~0~666} as command line. This means that it first runs a scalar loop for \SI{2}{\second}, then an \gls{AVX-512} \gls{FMA} loop for \SI{200}{\milli\second}, and finally another scalar loop for \SI{666}{\micro\second}. The last scalar loop's length is motivated by the fact that the hardware reclocking algorithm usually takes \SI[quotient-mode=fraction]{2/3}{\milli\second} to return a core to its level~0 frequency as we found out through the analysis we conducted in \Cref{sec:analysis}, thus, this should expose worst-case behavior when using hardware reclocking. The other durations were selected mostly arbitrarily -- we only wanted to ensure that there is enough time for a core to reach its maximum frequency within the first scalar loop. We ran our tool with three configurations, each executed 1000 times: with hardware reclocking, with \textsc{avxfreq} reclocking, and with user-space-driven manual reclocking through \textsc{avxfreq}.

The first scalar loop achieves approximately similar median performance with all three configurations: $1260960775$ iterations with hardware reclocking, $1264414085$ with \textsc{avxfreq} (approx. \SI{0.27}{\percent} faster), and $1263630469.5$ with user-space-driven reclocking (approx. \SI{0.21}{\percent} faster). These deviations seem negligible and we consider them to be statistical noise.

\begin{figure}
	\begin{tikzpicture}[trim axis left]
		\sffamily
		\begin{axis}[
			ylabel={\gls{AVX-512} \gls{FMA} loop iterations},
			scale only axis,
			width=\textwidth,
			height=5cm,
			axis lines=left,
			ymin=72500000,
			ymax=84000000,
			xtick=\empty,
			xmin=-10,
			xmax=1010,
			legend cell align=left,
			legend style={at={(\textwidth-\textwidth/1020*10,0.5)}, anchor=east},
			axis y discontinuity=parallel
		]
			\addplot[only marks, color=kitblue] table {plots/staged_execution_0_0_2000000_200000_0_666_avx_hwp.csv}; %node[pos=1.0,below=0.5cm,anchor=east] {Hardware reclocking};
			\addplot[only marks, color=kitgreen] table {plots/staged_execution_0_0_2000000_200000_0_666_avx_manual.csv}; %node[pos=1.0,above=0.72cm,anchor=east] {User-space-driven};
			\addplot[only marks, color=kityellow] table {plots/staged_execution_0_0_2000000_200000_0_666_avx_avxfreq.csv}; %node[pos=1.0,above=0.67cm,anchor=east] {\textsc{avxfreq}};
			\legend{Hardware,User-space,\textsc{avxfreq}}
		\end{axis}
	\end{tikzpicture}
	\caption{Completed \gls{AVX-512} \gls{FMA} loop iterations within \SI{200}{\milli\second} when core frequency is controlled by the hardware, by the user-space program itself, and by our reimplementation \textsc{avxfreq}. Hardware reclocking yields the best performance, while user-space-controlled reclocking is slightly faster than \textsc{avxfreq}.}
	\label{fig:evaluation:results:stagedexecution:avxstage}
\end{figure}

For the following \gls{AVX-512} loop, we find more interesting differences: as shown in \Cref{fig:evaluation:results:stagedexecution:avxstage}, \textsc{avxfreq} performs about \SI{11.7}{\percent} worse than hardware reclocking, yet it does so in a very consistent manner across all runs. User-space-driven decisions yield a performance increase of around \SI{1}{\percent} compared to \textsc{avxfreq}, but still result in a \SI{10.7}{\percent} decrease compared to the hardware implementation.

Unlike the hardware algorithm, \textsc{avxfreq} requires two interrupts for conducting a switch from turbo license level~0 to level~2. Further, since we can not disable hardware reclocking entirely on our test system, the hardware will still induce a single, additional frequency switch (but only one, since the offsets for both levels are the same). User-space decisions being faster than \textsc{avxfreq} is not surprising though, given that they do not require any interrupts, but only a single system call that directly switches from level~0 to level~2 without a step to level~1 in between. Additionally, it seems plausible that the chip takes longer to apply frequency switches requested by software than it needs for turbo license level switches after internal hardware events. Putting it all together, we do not have an exhaustive and conclusive explanation for this behavior, and finding one remains future work.

\begin{figure}
	\begin{tikzpicture}[trim axis left]
		\sffamily
		\begin{axis}[
			ylabel={Scalar loop iterations},
			scale only axis,
			width=\textwidth,
			height=5cm,
			axis lines=left,
			ymin=225000,
			ymax=490000,
			xtick=\empty,
			xmin=-10,
			xmax=1010,
			legend cell align=left,
			legend style={at={(\textwidth-\textwidth/1020*10,0.75)}, anchor=east},
			axis y discontinuity=parallel
		]
			\addplot[only marks, color=kitblue, mark options={scale=0.5}] table {plots/staged_execution_0_0_2000000_200000_0_666_scalar2_hwp.csv}; %node[pos=1.0,below=0.5cm,anchor=east] {Hardware reclocking};
			\addplot[only marks, color=kitgreen, mark options={scale=0.5}] table {plots/staged_execution_0_0_2000000_200000_0_666_scalar2_manual.csv}; %node[pos=1.0,above=0.72cm,anchor=east] {User-space-driven};
			\addplot[only marks, color=kityellow, mark options={scale=0.5}] table {plots/staged_execution_0_0_2000000_200000_0_666_scalar2_avxfreq.csv}; %node[pos=1.0,above=0.67cm,anchor=east] {\textsc{avxfreq}};
			\legend{Hardware,User-space,\textsc{avxfreq}}
		\end{axis}
	\end{tikzpicture}
	\caption{Completed scalar loop iterations within \SI{666}{\micro\second} when core frequency is controlled by the hardware, by the user-space program itself, and by our reimplementation \textsc{avxfreq}. Again, hardware is faster than \textsc{avxfreq}, but on median user-space reclocking outperforms both, with the caveat of high variance.}
	\label{fig:evaluation:results:stagedexecution:scalarstage2}
\end{figure}

\Cref{fig:evaluation:results:stagedexecution:scalarstage2} finally depicts the completed iterations of the last scalar loop for the three different setups. The hardware implementation reaches a median of $298849$ iterations, \textsc{avxfreq} is again about \SI{12.4}{\percent} slower with a median of $261738.5$. Compared to these, user-space manual reclocking shows extremely varying performance, however, in the median case it outperforms the hardware reclocking with $305805.5$, equaling a \SI{2.3}{\percent} increase in comparison. Again, we are unsure about the precise source of the high variance and can only generally attribute it to the arguments mentioned in the previous paragraph. Yet it is intriguing that we only experience this variance when upclocking, but not when downclocking. However, from the \gls{PCU}'s perspective this might make sense: downclocking can only reduce the chip's power consumption, while upclocking will most certainly increase it. Thus, it is likely that a requested frequency reduction may be granted instantaneously, while a frequency increase requires the \gls{PCU} to reevaluate the global load situation as it must ensure at all times that the processor's power consumption never exceeds its \gls{TDP}.