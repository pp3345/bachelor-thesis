\chapter{Evaluation}
\label{sec:evaluation}

We have presented the design of \textsc{avxfreq}, a software-based reimplementation of a partial model of Intel's \gls{AVX} reclocking algorithm, in the previous chapter. Further, we have described how \textsc{avxfreq} can be leveraged to allow user-space programs to choose the applied turbo license levels themselves. In this chapter, we will evaluate these implementations and measure how well our prototype reflects Intel's hardware implementation. For this purpose, we want to leverage the analysis framework we presented in \Cref{sec:analysis}. However, both \textsc{avxfreq} and our analysis framework need to make use of the \gls{PMU} and can not do so concurrently. Further, to evaluate \textsc{avxfreq}, the analysis system needs to obtain the virtual license levels from \textsc{avxfreq} instead of reading the license levels via the \gls{PMU}. For this reason, we will present modifications to \textsc{avxfreq} itself as well as the analysis tool that allow us to use the latter in order to compare \textsc{avxfreq}'s behavior to what Intel does and to the simplified algorithm we wanted to reflect. For the user-space-driven decisions we will describe the design of a simple program that simulates heterogeneous workloads with both scalar and \gls{AVX} instructions. Finally, we will present and discuss the results obtained from executing tests with these tools.

\section{Methodology and Design}
\label{sec:evaluation:methodology}

One of the aims of this chapter is to show how well \textsc{avxfreq} implements the model it is supposed to reflect. First, we want to verify whether frequency reductions are triggered in the right situations, i.e., a switch to license level~1 happens when a single \SI[number-unit-product=-]{512}{\bit} floating-point vector instruction was executed, and a second switch to level~2 is triggered when, on average, at least one of these instructions is executed over a period of \SI{100}{\micro\second}. Second, as soon as none of the monitored vector instructions were executed over period of \SI[quotient-mode=fraction]{2/3}{\milli\second}, \textsc{avxfreq} should revert to license level~0. As described in \Cref{sec:analysis:design:measurementmodes}, the \texttt{DOWNCLOCK}, \texttt{UPCLOCK}, and \texttt{REQUIRED\_INSTRUCTIONS} measurement modes of our analysis framework are capable of measuring these values. For this reason, we will modify our analysis framework and \textsc{avxfreq} to talk to each other, so that, in turn, we can leverage the analysis system.

The second aim of this evaluation is to measure the performance penalty incurred by \textsc{avxfreq} compared to Intel's hardware and implementation and to explore whether an improvement for heterogeneous workloads is generally possible with alternative \gls{AVX} reclocking governors. For this reason, we presented an interface in \Cref{sec:design:userspacedrivendecisions} that allows for optimal reclocking by handing control over license levels to user-space. To conduct our evaluation, we will build a tool that alternates between phases of configurable time periods consisting of either \gls{AVX} or scalar code. Using this tool, we will measure the throughput in an \gls{AVX} phase as well as an immediately following scalar phase with a length of \SI{666}{\micro\second} -- this is approximately the delay applied before switching from license levels 1 or 2 to 0 as shown in our analysis, hence this should generate worst-case results with hardware reclocking. We then compare results obtained with the hardware implementation, with \textsc{avxfreq} reclocking, and with user-space-governed license level switches.

\subsection{AVXFreq}
\label{sec:evaluation:design:avxfreq}

We want to compare performance counter values using our analysis framework from \Cref{sec:analysis} -- in the best case, running it with \textsc{avxfreq} should deliver similar results to what is seen with hardware reclocking. However, our framework relies on using the processor's \gls{PMU} (as described in \Cref{sec:analysis:design:pmu}) just like \textsc{avxfreq} does, and due to the limited amount of available performance counters they may not use the \gls{PMU} concurrently. Further, hardware-side performance events that count cycles a processor's core spends in a specific turbo license level do not make sense anymore when \textsc{avxfreq} is enabled as the license levels are now emulated by software.

To circumvent these obstacles, we adapted \textsc{avxfreq} and our analysis framework to work together to emulate the most important performance events in software: the idea is that \textsc{avxfreq} already uses some of the performance events that the analysis system would need anyway and that it is perfectly capable of counting cycles spent in different states. Therefore, we let \textsc{avxfreq} do the bookkeeping and provide our framework's kernel component with software-generated events instead of interrupts in a manner that is completely transparent to the user-space part.

\textsc{avxfreq} in itself only needs one programmable performance counter to count retired \SI[number-unit-product=-]{512}{\bit} floating-point double-precision packed vector operations in addition to a fixed counter to count core cycles. For this purpose, \textsc{avxfreq} keeps raw counters in software that are updated using the values provided by the \gls{PMU} whenever any event triggers \textsc{avxfreq} code to be run. As \textsc{avxfreq} exclusively manages these counters, it needs to provide them to the analysis framework's kernel module -- other counters may be managed by the module itself. To enable interaction between the analysis framework's kernel module and \textsc{avxfreq}, we defined and implemented the following kernel-internal interface:

\begin{itemize}
	\item \mintinline{c}{bool avxfreq_is_enabled(void)} \\
		Returns \mintinline{c}{true} if \textsc{avxfreq} was enabled during system boot, \mintinline{c}{false} otherwise. Only if \textsc{avxfreq} is enabled, the analysis framework needs to behave differently than in \Cref{sec:analysis}.
	\item \mintinline{c}{avxfreq_counters *avxfreq_get_counters(int cpu)} \\
		\mintinline{c}{avxfreq_counters} is a C language \mintinline{c}{struct} that contains the raw counters for the values defined above. One instance is defined for every core in the system. Given a core number, this method returns a pointer to the instance for the respective core.
	\item \mintinline{c}{void avxfreq_reset_cycle_counter(void)} \\
		This method resets the current core's cycle fixed counter. % TODO why the fuck do we need this
	\item \begin{minted}[autogobble]{c}
			void avxfreq_set_license_transition_listener
			     (void (*listener)(u8 from, u8 to))
		\end{minted}
		\vspace{-0.45cm} % TODO what's the correct value here?
		Using this method, the analysis kernel module can hook into \textsc{avxfreq} and receive notifications whenever the applied virtual license level changes. The argument provided is a pointer to a function that takes two arguments: the previous virtual license level and the new one.
\end{itemize}

Using this interface, we modify our kernel component as follows: when loaded, it calls \texttt{avxfreq\_is\_enabled()} to check whether \textsc{avxfreq} is active. If so, it will not reconfigure the \gls{APIC} to take over handling of \gls{PMU} interrupts, but rather call \texttt{avxfreq\_set\_license\_transition\_listener()} to receive updates from \textsc{avxfreq} about license level switches and \texttt{avxfreq\_get\_counters()} once for every \gls{CPU} core in the system to locally store pointers to all counters provided by \textsc{avxfreq}. In addition to the raw counters, we added a local structure that contains derived counters, e.g., \textsc{avxfreq} only counts core cycles since they were last reset, but we also need to be able to distinguish cycles spent in the different license levels.

Whenever the module needs to write to \gls{PMU} \glspl{MSR} after being instructed to do so by the user-space component (e.g., during the \texttt{SETUP ioctl()}), it stores updated performance counter configurations in an array, so that it can later map them to software counters from \textsc{avxfreq}. Direct writes to performance counters are remapped to the corresponding software counters, unless there is no software counter available for them -- in this case, they are routed to the \glspl{MSR} just like when \textsc{avxfreq} is disabled. Similar action is taken when trying to read performance counters. This way, \gls{PMU} configuration remains fully transparent to user-space and hardware counters are automatically remapped to software counters when required without need for further intervention.

Interrupt handling is, as previously mentioned, disabled in case \textsc{avxfreq} is enabled. Instead, when the \texttt{avxfreq\_license\_transition\_listener} is called, it will update all local software counters with the values currently set in \textsc{avxfreq}'s counters and trigger the interrupt handler if the license level transition reflects one for which user-space requested an interrupt. Again, using this mechanism, we substitute hardware interrupts with software-based events in a manner that is transparent to user-space.

The interrupt handler itself (which now does not necessarily handle \emph{interrupts}) only needs a very small modification to work in this scenario: it must not reset the interrupt state in hardware, both for the \gls{PMU} and the \gls{APIC}. Apart from that, all interrupt actions we defined in \Cref{sec:analysis:design:kernel} work the same way as before.

As shown, these modifications allow us to fully employ our analysis framework with all its features and without drawbacks in combination with \textsc{avxfreq} enabled.

\subsection{Multi-Phase Execution}
\label{sec:evaluation:design:stagedexecution}

In this section, to create worst-case scenarios that we can use to measure the maximum negative impact of \textsc{AVX} reclocking on heterogeneous workloads, we will present a tool that allows to arbitrarily simulate such workloads. Further, we can use it to evaluate any improvements achieved with modified reclocking algorithms, e.g., the user-space-driven decisions we presented in \Cref{sec:design:userspacedrivendecisions} -- note that the worst case scenario for hardware reclocking would equal the best case for user-space-governed reclocking in terms of theoretically possible improvements. In addition, the tool may be employed to measure the impact of \textsc{avxfreq} on performance in comparison with the hardware implementation in Intel processors. We can not use real-life workloads as they would most certainly make use of instruction types that are not considered by \textsc{avxfreq}'s reclocking mechanism, and second, because likely none of them would expose worst-case behavior.

The idea is to executes multiple \emph{phases} of arbitrary duration, where each phase represents a different load type: scalar, \gls{AVX} instructions targeting turbo license level~1, and \gls{AVX} targeting level~2. We use \SI[number-unit-product=-]{512}{\bit} \gls{FMA} instructions (\texttt{vfmaddsub132pd}, precisely) for the two latter phase types, where the level~1 variant uses code that exhibits register pressure to limit throughput, whereas the level~2 variant is designed to achieve full utilization of the available \gls{AVX} execution units. Scalar phase only use simple increment and jump instructions. This was not an arbitrary choice: \textsc{avxfreq}, by design, only tries to resemble the hardware's implementation for \SI[number-unit-product=-]{512}{\bit} \gls{FMA} instructions.

The user may pass an arbitrary amount of arguments to the program, each representing a phase and how long it should be executed in \si{\micro\second}. For example, the first argument represents a run of a scalar phase, the second one a level~1 phase, the third one a level~2 phase, the fourth one a scalar phase again, and so on. Our implementation then runs each of the passed phases one after another for the given duration. During startup, the program spawns a separate thread and binds it to a specific core to ensure the process scheduler does not migrate the thread between cores and enables the \texttt{SCHED\_RR} scheduling policy to engange real-time execution -- just like our analysis framework's user-space component (see \Cref{sec:analysis:design:userspace}). This thread then executes each phase by running a loop where each iteration contains a single instruction corresponding to the phase type as described above. We enforce the configured time windows for each phase by having the main thread sleep for this duration and notify the executing thread after it has passed. Performance is then measured by counting how many iterations of the loop are executed within the phase's time window.

Additionally, in order to implement the user-space-driven decisions as mentioned above, we have added a command line option which instructs the tool to make use of the interface we presented in \Cref{sec:design:userspacedrivendecisions} to set license levels according to the phase that is to be executed.

\begin{figure*}
	\centering
	\begin{tikzpicture}[font=\scriptsize]
		\sffamily
		\pgfmathsetmacro{\linelength}{\textwidth-3cm}
		\begin{axis}[
			scale only axis,
			width=\linelength,
			height=1cm,
			xlabel={\si{\micro\second}},
			axis y line=none,
			axis x line=bottom,
			xmin=0,
			xmax=1000,
		]
			\addplot[only marks, color=white] coordinates {(0,0) (1000,0)}; % this is necessary because, well, i don't know
		\end{axis}

		\pgfmathsetmacro{\barheight}{0.2}
		\pgfmathsetmacro{\linemargin}{1.2}
		\pgfmathsetmacro{\linepadding}{0.1}
		\pgfmathsetmacro{\linedistance}{0.7}
		\pgfmathsetmacro{\linezeroy}{2.1}
		\pgfmathsetmacro{\lineoney}{\linezeroy-\linedistance}
		\pgfmathsetmacro{\linetwoy}{\lineoney-\linedistance}
		\pgfmathsetmacro{\levelheight}{0.2}

		% level 0
		\node[color=kitblue, minimum height=\barheight cm, text centered] at (-\linemargin, \linezeroy) {Heavy \SI[number-unit-product=-]{512}{\bit} AVX};
		\draw[color=kitdarkgrey] (0, \linezeroy) -- ++(\linelength pt, 0);

		% level 1
		\node[color=kitblue, minimum height=\barheight cm, text centered] at (-\linemargin, \lineoney) {Light \SI[number-unit-product=-]{512}{\bit} AVX};
		\draw[color=kitdarkgrey] (0, \lineoney) -- ++(\linelength pt, 0);

		% level 2
		\node[color=kitblue, minimum height=\barheight cm, text centered] at (-\linemargin, \linetwoy) {Scalar};
		\draw[color=kitdarkgrey] (0, \linetwoy) -- ++(\linelength pt, 0);

		\pgfmathsetmacro{\baronelength}{\linelength*0.2}
		\pgfmathsetmacro{\bartwolength}{\linelength*0.15}
		\pgfmathsetmacro{\barthreelength}{\linelength*0.4}
		\pgfmathsetmacro{\barfourlength}{\linelength*0.1}
		\pgfmathsetmacro{\bartwox}{0+\baronelength}
		\pgfmathsetmacro{\barthreex}{\bartwox+\bartwolength}
		\pgfmathsetmacro{\barfourx}{\barthreex+\barthreelength}
		\draw[fill=kitgreen, color=kitgreen] (0, 2.0) rectangle ++(\baronelength pt, \levelheight cm);
		\draw[fill=kitgreen, color=kitgreen] (\bartwox pt, 1.3) rectangle ++(\bartwolength pt, \levelheight cm);
		\draw[fill=kitgreen, color=kitgreen] (\barthreex pt, 0.6) rectangle ++(\barthreelength pt, \levelheight cm);
		\draw[fill=kitgreen, color=kitgreen] (\barfourx pt, 1.3) rectangle ++(\barfourlength pt, \levelheight cm);
	\end{tikzpicture}
	\caption{Exemplary run of our multi-phase execution tool when called with \texttt{200~150~400~0~100} as command line. Heavy \SI[number-unit-product=-]{512}{\bit} \gls{AVX} instructions are executed for a duration of \SI{200}{\micro\second}, then light ones for \SI{150}{\micro\second}, then purely scalar instructions for \SI{400}{\micro\second}, and finally a last phase with heavy \SI[number-unit-product=-]{512}{\bit} \gls{AVX} again for another \SI{100}{\micro\second}.}
	\label{fig:evaluation:design:stagedexecution}
\end{figure*}

\section{Results}
\label{sec:evaluation:results}

For our evaluation we used the same system that we previously described in \Cref{sec:analysis:results:systemsetup} with an Intel Core i9-7940X processor installed on an ASUS TUF X299 MARK 2 motherboard. However, to make \textsc{avxfreq} work correctly, we needed to modify some settings in the system's \gls{UEFI} configuration: first, we had to disable Intel \gls{HWP} as the firmware enables it by default, whereas it is necessary to have \gls{HWP} disabled for \textsc{avxfreq} to allow it to exercise full control over the processor's \glspl{P-state} as outlined in \Cref{sec:design:reimplementation:intel_pstate}. Second, while it is not possible to disable \gls{AVX}-induced reclocking entirely, we have set the reclocking offsets to 1 for both license levels to minimize the impact by the hardware's implementation -- as pointed out in \Cref{sec:design:reimplementation:avxfreq}, this is necessary for \textsc{avxfreq} to work properly and this is the reason we chose that specific motherboard. This, though, also means that with \textsc{avxfreq} enabled, there are two reclocking responses to the execution of \gls{AVX} code: by the processor itself and by \textsc{avxfreq}. While the resulting final frequencies still equal the ones reached with unmodified hardware reclocking (i.e., \SI{3.4}{\giga\hertz} for level~1, and \SI{2.8}{\giga\hertz} for level~2), frequency changes potentially happen twice.

In this section, we will present the results obtained by executing tests using the modified \textsc{avxfreq} and analysis framework implementations as well as the multi-phase execution tool we presented in the previous section.

\subsection{AVXFreq}
\label{sec:evaluation:results:avxfreq}

As discussed in \Cref{sec:analysis:design:measurementmodes}, our analysis framework provides multiple measurement modes: \texttt{DOWNCLOCK} to measure the time taken until a frequency reduction is completed, \texttt{UPCLOCK} for the reverse process, \texttt{PRE\_THROTTLE\_TIME} to determine the time before a core is throttled after \gls{AVX} execution has begun, and \texttt{REQUIRED\_INSTRUCTIONS} to find out how many instructions exactly are required to trigger the reclocking. To verify how well \textsc{avxfreq} implements the simplified reclocking algorithm we wanted to reflect, we executed tests using all these modes with \textsc{avxfreq} and the above-mentioned system configuration, with the exception of the \texttt{PRE\_THROTTLE\_TIME} mode: as the throttling is purely controlled by the hardware itself and not affected by any system configuration or \textsc{avxfreq}, it makes no sense to run this mode again here. Further, \textsc{avxfreq} is designed to only resemble the \gls{CPU}'s behavior when executing \SI[number-unit-product=-]{512}{\bit} \gls{FMA} instructions, with turbo frequencies enabled, and with only one active core, and as such, we only ran tests under these specific conditions.

In the case of transitions from level~0 to level~1 measured using the \texttt{DOWNCLOCK} and \texttt{REQUIRED\_INSTRUCTIONS} modes, there are no surprises: \textsc{avxfreq} was designed to immediately switch to level~1 as soon as a single \SI[number-unit-product=-]{512}{\bit} double-precision floating point vector instruction is retired and we find this to essentially hold true. The amount of completed operations in the \texttt{DOWNCLOCK} test for the unrolled case (i.e., when both \gls{AVX-512} units of the core are utilized in parallel) varies between 18 and 34 (equaling 9 and 17 instructions, respectively, as a \gls{FMA} instruction consists of two operations) before \textsc{avxfreq} invokes and reports the transition to level~1. Given that the hardware implementation reacts precisely after the first instruction there is a small deviation here, however, we believe this is caused by \gls{PMU} interrupts being slightly delayed and consider it to be negligible. This assumption is supported by the fact that the \texttt{REQUIRED\_INSTRUCTIONS} test generally shows one single instruction to be sufficient to trigger a level transition.

Transitions to level~2 examined using the same measurement modes, however, show a larger deviation between the hardware's behavior and the our results: the median of the time needed here lies at \SI{103.8}{\micro\second}, whereas Intel's implementation will immediately start a switch to level~2 after the first heavy instruction was executed in level~1. Looking at how \textsc{avxfreq} was implemented, though, this is not surprising: after having moved to level~1, \textsc{avxfreq} will trigger interrupts every \SI{100}{\micro\second} to check whether any of the measured instructions were executed in meantime and then either triggers a switch to level~2 if the throughput is high enough or counts towards the upclocking timer if no instructions were executed. Thus, a transition from level~1 to level~2 may only occur at least \SI{100}{\micro\second} after the transition from level~0 to level~1. Delays caused through context switches and kernel code could provide a plausible explanation for the remaining $\sim$\SI{4}{\micro\second}.

Tests using the \texttt{UPCLOCK} mode show that \textsc{avxfreq} behaves as designed: the average and median times taken to revert a frequency reduction lie at about \SI{665}{\micro\second}, which is slightly below the \SI{666}{\micro\second} we were aiming at. All results are distributed very homogeneously around this value. Again, we attribute the small deviation to time taken by context switches and code executed within the kernel.

\subsection{Overhead and Reclocking Optimization Potential}
\label{sec:evaluation:results:stagedexecution}

We designed our multi-phase execution tool with the goal of being able to determine a baseline for what optimized reclocking algorithms could achieve in the best case by using user-space-driven decisions as we described in \Cref{sec:design:userspacedrivendecisions}. Further, this tool allows us additional comparisons between \textsc{avxfreq} and Intel's hardware implementation -- in the previous section, we essentially measured how well our reimplementation reflects the model we wanted to achieve, whereas here, we can verify that \textsc{avxfreq} provides similar performance to the hardware implementation.

To obtain the following results, we ran our multi-phase execution tool using \texttt{0~0~2000000~200000~0~666} as command line. This means that it first runs a scalar loop for \SI{2}{\second}, then an \gls{AVX-512} \gls{FMA} loop for \SI{200}{\milli\second}, and finally another scalar loop for \SI{666}{\micro\second}. The last scalar loop's length is motivated by the fact that the hardware reclocking algorithm usually takes \SI[quotient-mode=fraction]{2/3}{\milli\second} to return a core to its level~0 frequency as we found out through the analysis we conducted in \Cref{sec:analysis}, thus, this should expose worst-case behavior when using hardware reclocking. The other durations were selected mostly arbitrarily -- we only wanted to ensure that there is enough time for a core to reach its maximum frequency within the first scalar loop. We ran our tool with three configurations, each executed 1000 times: with hardware reclocking, with \textsc{avxfreq} reclocking, and with user-space-driven manual reclocking through \textsc{avxfreq}.

The first scalar loop achieves approximately similar median performance with all three configurations: $1261.4\times10^6$ iterations with hardware reclocking, $1265.3\times10^6$ with \textsc{avxfreq} (approx. \SI{0.31}{\percent} faster), and $1262.4\times10^6$ with user-space-driven reclocking (approx. \SI{0.08}{\percent} faster). These deviations seem negligible and we consider them to be statistical noise as \textsc{avxfreq} is only waiting for an interrupt and does not actively do anything.

\begin{figure}
	\begin{tikzpicture}[trim axis left]
		\sffamily
		\begin{axis}[
			xlabel={Runs ($n=1000$)},
			ylabel={AVX-512 FMA Loop Iterations},
			scale only axis,
			width=\textwidth,
			height=5cm,
			axis lines=left,
			ymin=35050000,
			ymax=36200000,
			xtick=\empty,
			xmin=-10,
			xmax=1010,
			legend cell align=left,
			legend style={at={(\textwidth-\textwidth/1020*10,0.25)}, anchor=east},
			axis y discontinuity=parallel
		]
			\addplot[only marks, color=kitblue, mark options={scale=0.5}] table {plots/staged_execution_0_0_2000000_200000_0_666_avx_hwp.csv}; %node[pos=1.0,below=0.5cm,anchor=east] {Hardware reclocking};
			\addplot[only marks, color=kitgreen, mark options={scale=0.5}] table {plots/staged_execution_0_0_2000000_200000_0_666_avx_manual.csv}; %node[pos=1.0,above=0.72cm,anchor=east] {User-space-driven};
			\addplot[only marks, color=kityellow, mark options={scale=0.5}] table {plots/staged_execution_0_0_2000000_200000_0_666_avx_avxfreq.csv}; %node[pos=1.0,above=0.67cm,anchor=east] {\textsc{avxfreq}};
			\legend{Hardware,User-space,\textsc{avxfreq}}
		\end{axis}
	\end{tikzpicture}
	\caption{Completed \gls{AVX-512} \gls{FMA} loop iterations within \SI{200}{\milli\second} when core frequency is controlled by the hardware, by the user-space program itself, and by our reimplementation \textsc{avxfreq}. Hardware as well as user-space reclocking yield the best performance, while \textsc{avxfreq} is only slightly slower with a difference of $\sim$\SI{1}{\percent}.}
	\label{fig:evaluation:results:stagedexecution:avxstage}
\end{figure}

For the following heavy \gls{AVX-512} loop, we find more interesting differences: as shown in \Cref{fig:evaluation:results:stagedexecution:avxstage}, \textsc{avxfreq} performs about \SI{0.9}{\percent} worse than hardware reclocking, yet it does so in a very consistent manner across all runs. User-space-driven decisions yield a performance increase of around \SI{1}{\percent} compared to \textsc{avxfreq} and reach similar performance as hardware reclocking -- surprisingly, with lower variance.

The overhead incurred by \textsc{avxfreq} is easily explained: unlike the hardware algorithm, \textsc{avxfreq} requires two interrupts for conducting a switch from turbo license level~0 to level~2. Further, since we can not disable hardware reclocking entirely on our test system, the hardware will still induce a single additional frequency switch (but only one, since the offsets for both levels are the same). User-space decisions being faster than \textsc{avxfreq} is not surprising, given that they do not require any interrupts, but only a single system call that directly switches from level~0 to level~2 without a step to level~1 in between.

\begin{figure}
	\begin{tikzpicture}[trim axis left]
		\sffamily
		\begin{axis}[
			xlabel={Runs ($n=1000$)},
			ylabel={Scalar Loop Iterations},
			scale only axis,
			width=\textwidth,
			height=5cm,
			axis lines=left,
			ymin=230000,
			ymax=530000,
			xtick=\empty,
			xmin=-10,
			xmax=1010,
			legend cell align=left,
			legend style={at={(\textwidth-\textwidth/1020*10,0.75)}, anchor=east},
			axis y discontinuity=parallel
		]
			\addplot[only marks, color=kitblue, mark options={scale=0.5}] table {plots/staged_execution_0_0_2000000_200000_0_666_scalar2_hwp.csv}; %node[pos=1.0,below=0.5cm,anchor=east] {Hardware reclocking};
			\addplot[only marks, color=kitgreen, mark options={scale=0.5}] table {plots/staged_execution_0_0_2000000_200000_0_666_scalar2_manual.csv}; %node[pos=1.0,above=0.72cm,anchor=east] {User-space-driven};
			\addplot[only marks, color=kityellow, mark options={scale=0.5}] table {plots/staged_execution_0_0_2000000_200000_0_666_scalar2_avxfreq.csv}; %node[pos=1.0,above=0.67cm,anchor=east] {\textsc{avxfreq}};
			\legend{Hardware,User-space,\textsc{avxfreq}}
		\end{axis}
	\end{tikzpicture}
	\caption{Completed scalar loop iterations within \SI{666}{\micro\second} after previous \gls{AVX-512} \gls{FMA} execution when core frequency is controlled by the hardware, by the user-space program itself, and by our reimplementation \textsc{avxfreq}. Again, hardware is faster than \textsc{avxfreq}, but in the median case user-space reclocking outperforms both, with the caveat of exhibiting high variance.}
	\label{fig:evaluation:results:stagedexecution:scalarstage2}
\end{figure}

\Cref{fig:evaluation:results:stagedexecution:scalarstage2} finally depicts the completed iterations of the last scalar loop for the three different setups. The hardware implementation reaches a median of $299004$ iterations, \textsc{avxfreq} is again about \SI{1.8}{\percent} slower with a median of $293518.5$. Compared to these results, user-space manual reclocking shows extremely varying performance. However, in most runs it outperforms hardware reclocking with a median of $323083.5$, equaling an \SI{8}{\percent} performance increase. Looking at the 99th percentile throughput of both cases, the improvement is even larger with about \SI{14.9}{\percent}. We are unsure about the precise source of the high variance, but it is intriguing that we only experience this variance when upclocking, but not when downclocking. However, looking at it from the \gls{PCU}'s perspective might yield an explanation: downclocking can only reduce the chip's power consumption, whereas upclocking will most certainly increase it. Thus, it is likely that a requested frequency reduction may be granted instantaneously, whereas a frequency increase requires the \gls{PCU} to reevaluate the global load situation as it must ensure at all times that the processor's power consumption never exceeds its \gls{TDP}. This issue would not occur with hardware reclocking as the hardware merely applies a negative offset to a frequency that was previously deemed to be in line with the electrical limitations, so it is clear that upon reverting a frequency reduction, the raised frequency can not incur a problem.

\section{Discussion}
\label{sec:evaluation:discussion}

In this thesis, our primary goal is to evaluate whether the idea of a software-based reimplementation of Intel's \gls{AVX} reclocking algorithm is a feasible and useful approach to finding ways of optimizing the frequency scaling behavior to achieve improved performance for heterogeneous workloads consisting of both scalar and vector parts. For this purpose, we have conducted a thorough analysis of the frequency scaling behavior of a current Intel processor while executing \gls{AVX} instructions in \Cref{sec:analysis} and then used the obtained information in \Cref{sec:design} to build \textsc{avxfreq}, our \gls{Linux}-based reimplementation, on top of which we implemented a way for user-space programs to use their knowledge of what they are going to do next in order to control \gls{AVX} reclocking themselves as a simple optimization approach. We then compared \textsc{avxfreq}'s behavior to the hardware's and tested the performance of user-space-based reclocking. Putting it all together, our answer to the question \enquote{Is reimplementation in software a sensible approach for optimizing \gls{DVFS}?} is \enquote{maybe.}

The achievable quality of a reimplementation is limited by several factors: first, it can neither determine nor control the power gating status of vector registers and vector execution units which, on the one hand, seems to be a relevant factor in the downclocking behavior and, on the other hand, would be necessary to do manual upclocking without incurring an unnecessary waste of energy as the units remain powered even after raising the frequency through software means. Second, it is impossible to start measuring time as soon as the processor's pipeline is free of \gls{AVX} instructions, albeit that would be necessary for a precise replication of the observed upclocking behavior. Third, by using the available performance events, we can only measure executed vector floating-point instructions but not integer ones. And finally, with the hardware in our test system, we were unable to completely disable the processor's own \gls{AVX}-induced reclocking, but rather had it reduced to a minimum.

However, some of these issues do not seem too bad: for the purpose of testing alternative algorithms that solely impact when and how the frequency is changed, it is not important to control power gating as long as execution does not become unstable and the chip's total power consumption is below its \gls{TDP} -- otherwise, unnecessarily powered units may have an impact on the achievable frequency. Nevertheless, this issue can be circumvented by not utilizing all cores or by raising the applied \gls{TDP} (on modern Intel \glspl{CPU}, the \gls{TDP} can be configured through an \gls{MSR} \cite{intelsdmsysprogguide}). Further, while missing hardware support for measuring executed integer vector instructions hinders us from being able to test real-world workloads, it is still possible to conduct tests with synthetic programs. It is conceivable that, with some work, a synthetic workload may reflect a real-world one very accurately with respect to all factors relevant for \gls{AVX} reclocking (i.e., when are \gls{AVX} instructions executed and when not).

The other two remaining issues are the biggest ones in our view: any conceivable automatic (i.e., when decisions are not simply made by user-space) reclocking algorithm would need precise information about when a vector execution streak has ended. While approximations as used in \textsc{avxfreq} are certainly possible, they incur a trade-off between accuracy and performance due to the need for short timer interrupts and remain a major source of inaccuracy. However, an improvement may be achievable here using the upcoming \textit{Icelake} processor generation from Intel: these chips will feature an extended \gls{PEBS} interface compared to \textit{Skylake} \cite{intelsdmsysprogguide}. Using \gls{PEBS}, it is already possible on current generations to collect samples containing register states, \gls{TSC} values and other data every $n$ assertions of a performance event. However, on pre-\textit{Icelake} processors, \gls{PEBS} is only supported for a few specific performance events -- the ones required for counting vector instructions not among them. Starting with \textit{Icelake}, \gls{PEBS} will be made available for all defined events. Then, it would be possible to configure \gls{PEBS} to store the \gls{TSC} value every time a certain number of floating-point vector instructions has retired. This way, we could create approximations that are off by only up to a few hundred nanoseconds without incurring further overhead.

Further, while it is not possible to disable the hardware's \gls{AVX} reclocking in our current test setup, it could be worthwhile to put some reverse engineering effort into how the motherboard configures the processor's reclocking offsets. It seems plausible that this happens via an undocumented \gls{MSR} or a register of the \gls{PCIe} host bridge. Intel has not published a datasheet containing descriptions of the host bridge's registers for the processor we used in our system, however, such datasheets exist for other Intel processors (e.g., \cite{intel7thgendatasheetvol2}). Although these do not seem to contain a register that controls \gls{AVX} reclocking, again, it may be there but not publicly documented. By modifying the \gls{UEFI}'s settings, then dumping undocumented \gls{MSR} and host bridge register addresses and looking for differences it may be possible to find out how the \gls{UEFI} applies the configuration to the \gls{CPU}. In turn, it is conceivable there is a way to actually disable reclocking in hardware entirely. However, as long as we do not manage to achieve that, hardware reclocking even with minimal offsets remains another large obstacle for accurate evaluations of alternative algorithms.

Looking at our evaluation results from the previous section in this chapter, however, we have proven that improved performance for heterogeneous workloads by using better reclocking is indeed possible. Using a configuration where we exposed the worst-case behavior of the hardware (which is, in turn, the best case for user-space reclocking), we have reached a performance increase of nearly \SI{15}{\percent} in the 99th percentile (\SI{8}{\percent} median) for a scalar workload directly following an \gls{AVX-512} \gls{FMA} one. Unfortunately, we have also seen that software-controlled upclocking causes a high variance in performance, presumably because the \gls{PCU} can not grant requests for frequency raises immediately. Assuming that this theory is correct though, the 99th percentile improvement likely represents the performance that would be reached if implemented in hardware as this should reflect the case where upclocking had the smallest delay.

We therefore believe that, despite the aforementioned issues, it still seems plausible that an automatic reclocking algorithm for \gls{AVX} workloads with better performance than what is currently achieved using hardware-controlled reclocking may be implemented using the software approach we presented in this work. However, to prove this, we would need to actually build and evaluate such an implementation, but for now, this remains future work.